{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First ML Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load pizzas and reservations from `pizza.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video https://app.pluralsight.com/course-player?clipId=43702a8d-6b59-49f2-a67b-320dfc2aa58e\n",
    "# Module 3, Seciton Making Sense of Our Data at minute 2:00\n",
    "# skiprows = 1 means we will skip the 1st row as it is the header\n",
    "# unpack = True means we unpack the data into 2 separate columns\n",
    "# X, Y = means we are going to assign data in the 1st column from text file to X and the 2nd to Y\n",
    "# Both X and Y are array\n",
    "\n",
    "import numpy as np\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the reservations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.,  2., 14., 23., 13., 13.,  1., 18.,  7., 10., 26.,  3.,  3.,\n",
       "       21., 22.,  2., 27.,  6., 10., 18., 15.,  9., 26.,  8., 15., 10.,\n",
       "       21.,  5.,  6., 13.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the reservations data\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and these are the pizzas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 16., 32., 51., 27., 25., 16., 34., 22., 17., 29., 15., 15.,\n",
       "       32., 37., 13., 44., 16., 21., 37., 30., 26., 34., 23., 39., 27.,\n",
       "       37., 17., 18., 23.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y is the number of pizzas\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot pizzas and reservations with Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAETCAYAAAAh/OHhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHS5JREFUeJzt3X20XFWZ5/HvLzcoBJA3A0QgCSgKaCM93EYYtAXkTaQN0yOMztWJNt1ZI82Ib4NgbAXb0Nq+gLqabm83SFwEhFERVKZtXrURGw2CCgIGIckAgQRJAA2iSZ75Y+/iVoqqm3vq1q1zqur3WavWqbPrnFNPneTWU2fvs/dWRGBmZjZR08oOwMzMeosTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkVMr3sACQtB54GNgIbImJY0s7AFcBcYDlwSkSsLStGMzMbU5UrjiMj4qCIGM7rZwE3RMS+wA153czMKqAqiaPRPGBxfr4YOKnEWMzMrI7K7jku6UFgLRDAlyJiVNK6iNixbpu1EbFTk30XAAsAtt1224P322+/boVtZtYXbr/99scjYmaRfUpv4wAOj4hHJO0KXCfp3onuGBGjwCjA8PBwLF26dKpiNDPrS5JWFN2n9KqqiHgkL1cDVwGHAI9JmgWQl6vLi9DMzOqVmjgkbStp+9pz4FjgLuAaYH7ebD5wdTkRmplZo7KrqnYDrpJUi+WyiPhXST8GrpR0KrASOLnEGM3MrE6piSMiHgBe3aT818Abuh+RmZltSeltHGZm1lucOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zCpmyRKYOxemTUvLJUvKjshsc2XPOW5mdZYsgQULYP36tL5iRVoHGBkpLy6zer7iMKuQhQvHkkbN+vWp3KwqnDjMKmTlymLlZmVw4jCrkNmzi5WblcGJw6xCFi2CGTM2L5sxI5WbVYUTh1mFjIzA6CjMmQNSWo6OumHcqsV3VZlVzMiIE4VVm684zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK6QSiUPSkKQ7JH07r+8t6TZJyyRdIekFZcdoZmZJJRIHcAZwT936p4DzI2JfYC1wailRmZnZ85SeOCTtCbwJ+Je8LuAo4Gt5k8XASeVEZ2ZmjUpPHMAFwJnApry+C7AuIjbk9YeAPZrtKGmBpKWSlq5Zs2bqIzUzs3ITh6QTgdURcXt9cZNNo9n+ETEaEcMRMTxz5swpidGKWbIE5s6FadPScsmSsiMys04rez6Ow4E3SzoB2Bp4EekKZEdJ0/NVx57AIyXGaBO0ZAksWADr16f1FSvSOnh+CbN+UuoVR0ScHRF7RsRc4K3AjRExAtwEvCVvNh+4uqQQrYCFC8eSRs369anczPpHFdo4mvkQ8H5J95PaPC4qOR6bgJUri5WbWW8qu6rqORFxM3Bzfv4AcEiZ8Vhxs2en6qlm5WbWP6p6xWE9aNEimDFj87IZM1K5mfUPJw7rmJERGB2FOXNASsvRUTeMm/WbylRVWX8YGXGiMOt3vuIwM7NCnDis0jrdoXC847nzotnEuKrKKqvTHQrHOx6486LZRCmi6WgePWd4eDiWLl1adhjWQXPnNr+9d84cWL68s8eDzr6XWa+QdHtEDBfZx1ccVlmd7lDYzvHcedHs+dzGYZXVquNgux0Kxztep9/LrJ85cVhldbpD4XjHc+dFs4lz4rDK6nSHwvGO586LZhPnxnEzswHWTuO4rzjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDqu0fh14sF8/lw0GDzlildXpQQ6rol8/lw0O9+Owyur0IIdV0a+fy3qT+3FYX+n0IIdV0a+fywaHE4e1VHY9fL8OPNivn8sGhxOHNVWrh1+xAiLG6uG7mTz6deDBfv1cNjicOKyphQvHGm9r1q9P5d3SrwMP9uvnssHhxnFratq0dKXRSIJNm7ofj5lNDTeOW8e4Ht7MWnHisKa2VA9fdsN5u3o1brMqcQdAa6pW375wYbpNdPbslDRGRnq3A1uvxm1WNW7jsMJ6tQNbr8ZtNpXcxmFd0asd2Ho1brOqceKwwnq14bxX4zarGicOK6xXO7D1atxmVePEYYX1age2Xo3brGo61jguaSvgVcD6iLivIwctwI3jZmbFdaVxXNIpkq6UtHNd2UuBu4GlwC8kfUPSFm/1lbS1pB9J+qmkuyWdm8v3lnSbpGWSrpD0gqJxWjncT8Ks/7VTVfUXwH4R8URd2WeBlwE3AT8D5gHvmsCxngWOiohXAwcBx0s6FPgUcH5E7AusBU5tI07rsioMjGhmU6+dxHEA8OPaiqQXAScAV0bE0cAhwL1MIHFE8pu8ulV+BHAU8LVcvhg4qY04rcuqMDCimU29dhLHTGBV3fphpB7oXwWIiD8A1wEvncjBJA1JuhNYnff7FbAuIjbkTR4C9mix7wJJSyUtXbNmTRsfxTrJ/STMBkM7ieNpYIe69deTrhJuqSv7HbD9RA4WERsj4iBgT9LVyv7NNmux72hEDEfE8MyZMyfydjaF3E/CbDC0kziWAW+U9MLcaH0y8LOIeLxumzmkK4gJi4h1wM3AocCOdY3rewKPtBGndZn7SUycbyKwXtZO4hgF9iElkHvy84sbtnkN6S6rcUmaKWnH/Hwb4Oh8zJuAt+TN5gNXtxGndZn7SUyMbyKwXtdWPw5J5wF5XFGWAO+NfCBJRwHXA2dGxGe2cJwDSY3fQ6QkdmVEfFzSPqQ2k52BO4C3R8Sz4x3L/TisV3iwRauSdvpxdHx03Fx9tQ3w27oG7innxGG9wrMrWpVUYnTciPh9RDzZzaRh1kt8E4H1Oo9VZdZlvonAel1biUPSLEn/IOl+Sc9I2tjk4SsOsyZ8E4H1usJTx0raA/gRsBvpzqkXAitIw4fsk495J/Bk58I06y8jI04U1rvaueL4KLA7cHweYwrgyxGxHylxfJfUOP7nnQnRJqrTfQPc18DMmmkncRwH/GtEXN/4QkQ8ROoQuA1w7iRjswI63TfAfQ3MrJV2EsfubN65byMpUQCQBy28jjRCrnVJpwcY9ICFZtZKO4njKaB+foy1PH8QwidJgyFal3R6gEEPWGhmrbSTOFYAe9Wt/xQ4StIMAEnTgGNJo9pal3S6b4D7GphZK+0kjhuAI/NUsZCGDHkJcKukTwM/AF4JXNGZEK1eqwbrTvcNcF8DM2ul8O24wEWk6qkXA6si4lJJBwP/Czgwb/NVwF8xHVZrsK61PdQarGHs1s6FC1N10uzZ6Uu+3Vs+O308M+sfHRurStJM0u24yyPisY4ctIBBGKvKg+OZWae1M1ZVO1ccTUXEGsDT8E0hN1ibWRUUbuOQdLGkL0raeZxt5klqnKPDJqnbDdbuAGhmzbTTOP5O4DRSY/g+LbY5iDQBk3VQNxus3QHQzFppd3TcO0jtGT+U9J87GI+No5uD47kDoJm10m7iuAY4AdgauF7SKZ0LycYzMpIawjdtSsupusvJ7Slm1krb83HksaoOJzWIXybpQx2LykrnDoBm1sqkJnKKiLuA15B6j58naVTSUEcis1K5A6CZtTLpGQAj4lHgdcB3gL8ErgV2mOxxrVyebMjMWulIP46IWC9pHvB54HTgDZ04rpXLkw2ZWTPtDnK4rrEwkvcA7wc02cDMzKyaCieOiNg7Ir4wzusXkAY9bNXHw7agKh3vqhKHmVVLO3OOzwZ+FxGrx9nsGdJc5FbQRAYyHKQ4zKx62qmqWg48JOn0cbZ5H/BgWxENuKp0vKtKHGZWPe3eVTUEfF7S+Z0MxqrT8a4qcZhZ9bSbOC4AbgLOkHSVpG22tIM9X7M2hKp0vKtKHGZWPe0mjieB44EvA/OAmyXt1rGoBkCrQQRPOKEaHe/cAdDMWpnMkCMbIuJU4G+AYdKAh/t3LLI+16oN4dprq9Hxzh0AzayVwjMAStoEnBMRH68rextwMeluqpOB1wIfjYiuDT/SazMATpuWrjQaSWkAQzOzbmhnBsBJDzkCEBGXA8cCm0hDjpzYieP2s6loQ3C/CzPrho4kDoCI+HfgMGAlcHCnjtuvOt2G4ImXzKxb2kkc7wKubvZCRCwDDgX+CfjKJOLqe51uQ3C/CzPrlsJtHFXVa20cneY2EzNrR2ltHFY+97sws27Z4lhVki4GAvhwRDyW1yci8u264x17L1KV1u6khvXRiPi8pJ2BK4C5pCFOTomItRN834G0aNHmY0uB+12Y2dTYYlVVvv02gP0j4pd5fSJiS7fjSpoFzIqIn0jaHrgdOAl4J/BERHxS0lnAThEx7tS0g15VBakhfOHCNCzI7NkpabjfhZmNp52qqomMjrt3Xj7csD5pEbEKWJWfPy3pHmAPUm/0I/Jmi4GbAc9pvgWeeMnMumGLiSMiVoy33imS5gJ/DNwG7JaTChGxStKuLfZZACwAmO3KfDOzrijUOC5ptqT/KunPc/tER0jaDvg68N6IeGqi+0XEaEQMR8TwzJkzOxWOmZmNY8KJQ9JngAeAK4H/Azwo6dOTDUDSVqSksSQivpGLH8vtH7V2kPEmjTIzsy6aUOKQ9N8Zm0v8XuC+/Pz9eZyqtkgScBFwT0R8ru6la4D5+fl8WnQ4NDOz7pvoFcepwAbg6Ih4ZUQcABxHuoV23Ftut+Bw4B3AUZLuzI8TgE8Cx0haBhyT183MrAImOuf4gcA3I+KmWkFEXC/pasbufiosIm4hXbk084Z2j2tmZlNnolccO5GqpxrdC+zYuXD6j0esNbN+M9ErjmnAH5qU/4HWVwwDrzZiba03d23EWnB/CzPrXUVux+2P0RC7yCPWmlk/mugVB8A5ks5p9oKkjU2KIyKKHL/vrFxZrNzMrBcUueJQwcfAjLzbqh2j2yPWuj3FzLphQlcEETEwSaCo8doxujlirdtTzKxbPJHTJM2dm76kG82ZA8uXd2/E2i3FYWbWTDuj4zpxTFJVZt6rShxm1ls8A2CDVnX+nWwL2FI7RrfaHTwDoJl1TUT0xePggw+OepdeGjFjRkT6HZ4eM2ZEvPvdzcsvvTTa0up9Lr10/Nc6rZvvZWb9A1gaBb9v+7aqqlWd/9AQbGxy8/Bk2gJatWN0u93BMwCaWVFu46hLHK3q/FuZirYAtzuYWdW5jaNOq7r9oRazoE9FW4DbHcysH/Vt4li0KPWZqDdjRurb0Ky81rfitNNg+vR0VTB9elrvdAxT0Y/DzKxb+jZxjIzA6GhqT5DScnQULrywefnISEoS//iPY20gGzem9XaTR6sY3O5gZr2sb9s42jF9evOG86Eh2LBhUoc2M6skt3FMUrOkMV65mdkgcuKo06rhvFW5mdkgcuKoUxsUcKLlZmaDaKDny2h04YVpOTqaqqeGhlLSqJWbmZkTx/NceKEThZnZeAayqsoTHpmZtW/grjg84ZGZ2eQM3BXHwoWbz8gHaX3hwnLiMTPrNQOXOFauLFZuZmabG7jE4YEHzcwmZ+AShwceNDObnIFLHB540MxscgburipIScKJwsysPQN3xWFmZpPjxGFmZoU4cZiZWSFOHGZmVogTh5mZFVJq4pB0saTVku6qK9tZ0nWSluXlTmXGaGZmmyv7iuMS4PiGsrOAGyJiX+CGvG5mZhVRauKIiO8DTzQUzwMW5+eLgZO6GpSZmY2r7CuOZnaLiFUAeblrqw0lLZC0VNLSNWvWdC1AM7NBVsXEMWERMRoRwxExPHPmzLLDMTMbCFVMHI9JmgWQl6tLjsfMzOpUMXFcA8zPz+cDV5cYi5mZNSj7dtzLgR8Cr5D0kKRTgU8Cx0haBhyT183MrCJKHR03It7W4qU3dDUQMzObsCpWVZmZWYU5cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlZIZROHpOMl3SfpfklnlR2PmZkllUwckoaAfwDeCBwAvE3SAeVGZWZmUNHEARwC3B8RD0TE74GvAvNKjsnMzIDpZQfQwh7A/6tbfwh4TeNGkhYAC/Lqs5Lu6kJsveDFwONlB1ERPhdjfC7G+FyMeUXRHaqaONSkLJ5XEDEKjAJIWhoRw1MdWC/wuRjjczHG52KMz8UYSUuL7lPVqqqHgL3q1vcEHikpFjMzq1PVxPFjYF9Je0t6AfBW4JqSYzIzMypaVRURGySdDnwXGAIujoi7t7Db6NRH1jN8Lsb4XIzxuRjjczGm8LlQxPOaDszMzFqqalWVmZlVlBOHmZkV0vOJY9CHJpF0saTV9X1YJO0s6TpJy/JypzJj7AZJe0m6SdI9ku6WdEYuH8RzsbWkH0n6aT4X5+byvSXdls/FFfnGk4EgaUjSHZK+ndcH8lxIWi7p55LurN2G287fSE8nDg9NAsAlwPENZWcBN0TEvsANeb3fbQA+EBH7A4cCf53/LwziuXgWOCoiXg0cBBwv6VDgU8D5+VysBU4tMcZuOwO4p259kM/FkRFxUF0/lsJ/Iz2dOPDQJETE94EnGornAYvz88XASV0NqgQRsSoifpKfP036ktiDwTwXERG/yatb5UcARwFfy+UDcS4AJO0JvAn4l7wuBvRctFD4b6TXE0ezoUn2KCmWKtktIlZB+kIFdi05nq6SNBf4Y+A2BvRc5KqZO4HVwHXAr4B1EbEhbzJIfysXAGcCm/L6LgzuuQjg3yTdnodsgjb+RirZj6OACQ1NYoND0nbA14H3RsRT6cfl4ImIjcBBknYErgL2b7ZZd6PqPkknAqsj4nZJR9SKm2za9+ciOzwiHpG0K3CdpHvbOUivX3F4aJLmHpM0CyAvV5ccT1dI2oqUNJZExDdy8UCei5qIWAfcTGr32VFS7cfioPytHA68WdJyUlX2UaQrkEE8F0TEI3m5mvSD4hDa+Bvp9cThoUmauwaYn5/PB64uMZauyPXWFwH3RMTn6l4axHMxM19pIGkb4GhSm89NwFvyZgNxLiLi7IjYMyLmkr4fboyIEQbwXEjaVtL2tefAscBdtPE30vM9xyWdQPoFURuaZFHJIXWVpMuBI0jDRD8GfAz4JnAlMBtYCZwcEY0N6H1F0muBfwd+zlhd9odJ7RyDdi4OJDVyDpF+HF4ZER+XtA/pV/fOwB3A2yPi2fIi7a5cVfXBiDhxEM9F/sxX5dXpwGURsUjSLhT8G+n5xGFmZt3V61VVZmbWZU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmfULSJZIiD7liNmWcOKwr8hda/WOjpCck3SzpnRrUsUEKkHROPndHlB2LDbZeH6vKes+5ebkV8DLgvwCvB4aB08sKqk+cDXwSeLjsQKy/OXFYV0XEOfXrkg4Hvg+cJumzEfFgKYH1gTyy6aqy47D+56oqK1VE/AC4lzRi6cHNtpF0nKRrJT0u6VlJv5L06dp4TA3bHijp8jzT2bOS1kj6iaQL8iCI9dtOl3SapP+Q9JSk9XmWuNMlTWvYdm6uJrpE0svzrHGrJW2SdISkeyX9XtKLW3yGs/L+f11XdqSkUUm/yO//jKS7JH1M0tYN+y8nDScDcFN9tV/dNi3bOCSdIun7kp7M7/NzSWdLemGTbZfnx4x8nlfmc3m/pA81q1aU9GZJN0halbd9RNL3JJ3W7HxYb/MVh1VB7YvoD897QfooqXrrCeDbpJE7DwQ+CJwg6bCIeCpveyBpbKogDdz2IPAiUpXYacBHau+Rk8i3gOOA+4DLgN8BRwJfBF4DvKNJrC/N7/FLYAmwDfAUaWyo84C35f0b/Q+gNtlYzYeA/YBbge8AW5NGcz0HOELS0Xl4dEjjsZ1EqtZbDCxv8h5NSTqPVI31eP6cvyHNmnkecJykYyKi8dxvBfwb8BLg/5JmWDyJVBW2NWNVjijN6/Al4FHSOX2cNKfDgcC7gAsnGqv1iIjww48pf5C+zKNJ+Z8CG0nTnc5qeO3IvN+twI4Nr70zv3Z+Xdlnc9m8Ju+zEzCtbv2cvO0XgaG68iHSKLubHQeYW/sMwHlNjr9H/hxLm7z2J3m/rzeU70MeL66h/G/z9v+tobwW8xEtzvEl+fW5dWWH5bKVwO515dNJX/IBfLjhOMtz+bXANnXluwLr8mOruvLb87/frk1ienHZ//f86PzDVVXWVfnOoHMkLZJ0BXA96Yrjg5FnIavznrz8q0jzSjwnIi4B7gRGmrzNM40FEbE2IjblGKaRGuIfBd4XY7/qyc8/QPribHbsx6j7tV2338Ok+ZoPlvTKhpdrQ1Yvbtjngcjfrg0uyMvjmrxW1F/k5Sci4tG6995A+pybgL9sse97IuKZun1Wk4bc3gF4RcO2G2hyxRgRj7cfulWVq6qs2z7WsB7AqRHx5SbbHkb6MjpZ0slNXn8BMFPSLhHxa+AK4Azgm5K+RkpKP4iIXzXs93LS9KHLgI+0uBP4GZrPmvfTaD389iXAMaREcSaAxuaJWUP6Bf+cPCfCGaQ7y14ObM/ms9N1YjrT/5SXNza+EBG/lPQQsLekHRuS85MRcX+T49Wmat6prmwJ6Wrv7vxj4Huk875m8uFbFTlxWFdFhOC5L83DSNVC/yRpRUQ0frntQvo/2phsGm0H/DoifiTpdcBC0iQ978jvdR9wbkRcXndcgH23cOztmpQ92qSs5ipSe8fbJZ2dr15OzO93QYzNcV1rY7mRNAPbXaSkt4axX+0fA57XcN2GHfKy1d1Wq0jzMOxAqoKqWdd8c2qfYahWEBGfk/Q4qR3pPcB7gZD0PeB/R8TSNmO3inJVlZUiIn4bEdcDf0b6ElosaUbDZk8CayNCW3isqDvuDyPiRNIv4sNJ7QW7AZdJOrruuABXbeG4ezcLfZzP9AxpQpxZpCsPaFFNBcwjJY3FEfFHEbEgIhZGul35S63eow21z7p7i9dnNWzXloj4SkQcSkqSbyL9IPhT4LtK81tbH3HisFJFxM+AfybN+/y+hpf/A9ipSZvBRI77bETcGhEfZaytZF5e3kv6RX1o4y26HXBJXs7Pt+a+EfhZRNzZsN3L8vLrTY7x+hbHrrXFDLV4vZk78vKIxhckvYx03h9sbENqV0Ssi4hrI+KvSOdiZ+B1nTi2VYcTh1XBJ0i3wn5QUn3d+fl5+c+SXtK4k9IcyofWrb9O0g6N25GuOADWw3MNw18k/dr+gtK83I3HniXpgKIfJFK/lGWkJPVu0m2tlzTZdHleHtHwvvsAn2px+F/n5ewCIV2clx+RNLPufYaAz5C+Ay4qcLznkXS8pGbV3rUrjfWTOb5Vj9s4rHQR8bCkL5Eais8k9TkgIm6QdBbwd8AySdeS+mZsB8wh/TK/BTg+H+oDwLGSbgYeIPVXeCXpV/9aYLTubf8WeDXwP4E/k3QjaaiOXUltH4eT2kp+0cZH+ko+/t+Q2gQua7LNt4D7gfdL+iPSlcFsUpvId2ieHG4i3QX1d5JelT8TEfGJVoFExK2S/p50Xu/KNw38lnROXkU6f59u4zPW+yrwO0m3kBKiSFcZf0K6Vff6SR7fqqbs+4H9GIwHLfpx1L2+G+kL7bfAbg2vvZbUdvAIqRPdGtKtuJ8Dhuu2Oxb4MunL/sl8rPuALwBzmrynSA3oN5A6GP6elDxuAT4M7FW37dz8GS6ZwGedTapWCuBb42y3F+mOpIdJd3HdTfqCn573vbnJPm/Pn/2ZxnNKk34cda+9NX+up0lXd3eTEuPWTbZdDixvEfM5NPQlISXfq0jJen0+l3fkz7J92f/3/Oj8Q/kf3szMbELcxmFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaF/H/0KNUmlQCFhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This directive tells Jupyter to draw matplotlib plots inside the web page:\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Reservations\", fontsize=20)  # Print the X label\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)        # Print the Y label\n",
    "plt.axis([0, 50, 0, 50])                 # Both axes range from 0 to 50\n",
    "plt.plot(X, Y, \"bo\")                     # Plot the data as blue circles (that's what \"bo\" stands for)\n",
    "plt.show()                               # Visualize the diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're about to approximate these data with linear regression–that is, with a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of a line is `y = x * w + b`. Translate it to code, and you get the `predict()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we already found a line that approximates the points, and this line has `w = 2.1` and `b = 7.3`. How many pizzas should we expect to sell if we got 10 reservations? Here's the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(14, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass an entire NumPy array to `predict()` instead of a single number. NumPy automatically applies the multiplication and the sum inside `predict()` to all the elements in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.8, 18. , 20.4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([14, 5, 7])\n",
    "predict(X, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement training, we have to define a loss function–a measure of how wrong a line is at approximating the dataset. We'll use the “mean squared error” formula for the loss. This function takes the dataset (`X` and `Y`) and a line (`w` and `b`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    "    predictions = predict(X, w, b)\n",
    "    return np.average((predictions - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with the data from `pizza.txt` and two made-up values for the line's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.778666666666677"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, Y, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the all-important `train()` function. It takes a dataset, and it returns a line that approximates it. It also takes a number of iterations and a learning rate (`lr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w - lr, b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X, Y, w + lr, b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "            b += lr\n",
    "        else:\n",
    "            return w, b\n",
    "\n",
    "    raise Exception(\"Couldn't find a result within %d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to find a line that approximates our `pizza.txt` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.866667\n",
      "Iteration    1 => Loss: 804.820547\n",
      "Iteration    2 => Loss: 796.818187\n",
      "Iteration    3 => Loss: 788.859587\n",
      "Iteration    4 => Loss: 780.944747\n",
      "Iteration    5 => Loss: 773.073667\n",
      "Iteration    6 => Loss: 765.246347\n",
      "Iteration    7 => Loss: 757.462787\n",
      "Iteration    8 => Loss: 749.722987\n",
      "Iteration    9 => Loss: 742.026947\n",
      "Iteration   10 => Loss: 734.374667\n",
      "Iteration   11 => Loss: 726.766147\n",
      "Iteration   12 => Loss: 719.201387\n",
      "Iteration   13 => Loss: 711.680387\n",
      "Iteration   14 => Loss: 704.203147\n",
      "Iteration   15 => Loss: 696.769667\n",
      "Iteration   16 => Loss: 689.379947\n",
      "Iteration   17 => Loss: 682.033987\n",
      "Iteration   18 => Loss: 674.731787\n",
      "Iteration   19 => Loss: 667.473347\n",
      "Iteration   20 => Loss: 660.258667\n",
      "Iteration   21 => Loss: 653.087747\n",
      "Iteration   22 => Loss: 645.960587\n",
      "Iteration   23 => Loss: 638.877187\n",
      "Iteration   24 => Loss: 631.837547\n",
      "Iteration   25 => Loss: 624.841667\n",
      "Iteration   26 => Loss: 617.889547\n",
      "Iteration   27 => Loss: 610.981187\n",
      "Iteration   28 => Loss: 604.116587\n",
      "Iteration   29 => Loss: 597.295747\n",
      "Iteration   30 => Loss: 590.518667\n",
      "Iteration   31 => Loss: 583.785347\n",
      "Iteration   32 => Loss: 577.095787\n",
      "Iteration   33 => Loss: 570.449987\n",
      "Iteration   34 => Loss: 563.847947\n",
      "Iteration   35 => Loss: 557.289667\n",
      "Iteration   36 => Loss: 550.775147\n",
      "Iteration   37 => Loss: 544.304387\n",
      "Iteration   38 => Loss: 537.877387\n",
      "Iteration   39 => Loss: 531.494147\n",
      "Iteration   40 => Loss: 525.154667\n",
      "Iteration   41 => Loss: 518.858947\n",
      "Iteration   42 => Loss: 512.606987\n",
      "Iteration   43 => Loss: 506.398787\n",
      "Iteration   44 => Loss: 500.234347\n",
      "Iteration   45 => Loss: 494.113667\n",
      "Iteration   46 => Loss: 488.036747\n",
      "Iteration   47 => Loss: 482.003587\n",
      "Iteration   48 => Loss: 476.014187\n",
      "Iteration   49 => Loss: 470.068547\n",
      "Iteration   50 => Loss: 464.166667\n",
      "Iteration   51 => Loss: 458.308547\n",
      "Iteration   52 => Loss: 452.494187\n",
      "Iteration   53 => Loss: 446.723587\n",
      "Iteration   54 => Loss: 440.996747\n",
      "Iteration   55 => Loss: 435.313667\n",
      "Iteration   56 => Loss: 429.674347\n",
      "Iteration   57 => Loss: 424.078787\n",
      "Iteration   58 => Loss: 418.526987\n",
      "Iteration   59 => Loss: 413.018947\n",
      "Iteration   60 => Loss: 407.554667\n",
      "Iteration   61 => Loss: 402.134147\n",
      "Iteration   62 => Loss: 396.757387\n",
      "Iteration   63 => Loss: 391.424387\n",
      "Iteration   64 => Loss: 386.135147\n",
      "Iteration   65 => Loss: 380.889667\n",
      "Iteration   66 => Loss: 375.687947\n",
      "Iteration   67 => Loss: 370.529987\n",
      "Iteration   68 => Loss: 365.415787\n",
      "Iteration   69 => Loss: 360.345347\n",
      "Iteration   70 => Loss: 355.318667\n",
      "Iteration   71 => Loss: 350.335747\n",
      "Iteration   72 => Loss: 345.396587\n",
      "Iteration   73 => Loss: 340.501187\n",
      "Iteration   74 => Loss: 335.649547\n",
      "Iteration   75 => Loss: 330.841667\n",
      "Iteration   76 => Loss: 326.077547\n",
      "Iteration   77 => Loss: 321.357187\n",
      "Iteration   78 => Loss: 316.680587\n",
      "Iteration   79 => Loss: 312.047747\n",
      "Iteration   80 => Loss: 307.458667\n",
      "Iteration   81 => Loss: 302.913347\n",
      "Iteration   82 => Loss: 298.411787\n",
      "Iteration   83 => Loss: 293.953987\n",
      "Iteration   84 => Loss: 289.539947\n",
      "Iteration   85 => Loss: 285.169667\n",
      "Iteration   86 => Loss: 280.843147\n",
      "Iteration   87 => Loss: 276.560387\n",
      "Iteration   88 => Loss: 272.321387\n",
      "Iteration   89 => Loss: 268.126147\n",
      "Iteration   90 => Loss: 263.974667\n",
      "Iteration   91 => Loss: 259.866947\n",
      "Iteration   92 => Loss: 255.802987\n",
      "Iteration   93 => Loss: 251.782787\n",
      "Iteration   94 => Loss: 247.806347\n",
      "Iteration   95 => Loss: 243.873667\n",
      "Iteration   96 => Loss: 239.984747\n",
      "Iteration   97 => Loss: 236.139587\n",
      "Iteration   98 => Loss: 232.338187\n",
      "Iteration   99 => Loss: 228.580547\n",
      "Iteration  100 => Loss: 224.866667\n",
      "Iteration  101 => Loss: 221.196547\n",
      "Iteration  102 => Loss: 217.570187\n",
      "Iteration  103 => Loss: 213.987587\n",
      "Iteration  104 => Loss: 210.448747\n",
      "Iteration  105 => Loss: 206.953667\n",
      "Iteration  106 => Loss: 203.502347\n",
      "Iteration  107 => Loss: 200.094787\n",
      "Iteration  108 => Loss: 196.730987\n",
      "Iteration  109 => Loss: 193.410947\n",
      "Iteration  110 => Loss: 190.134667\n",
      "Iteration  111 => Loss: 186.902147\n",
      "Iteration  112 => Loss: 183.713387\n",
      "Iteration  113 => Loss: 180.568387\n",
      "Iteration  114 => Loss: 177.467147\n",
      "Iteration  115 => Loss: 174.409667\n",
      "Iteration  116 => Loss: 171.395947\n",
      "Iteration  117 => Loss: 168.425987\n",
      "Iteration  118 => Loss: 165.499787\n",
      "Iteration  119 => Loss: 162.617347\n",
      "Iteration  120 => Loss: 159.778667\n",
      "Iteration  121 => Loss: 156.983747\n",
      "Iteration  122 => Loss: 154.232587\n",
      "Iteration  123 => Loss: 151.525187\n",
      "Iteration  124 => Loss: 148.861547\n",
      "Iteration  125 => Loss: 146.241667\n",
      "Iteration  126 => Loss: 143.665547\n",
      "Iteration  127 => Loss: 141.133187\n",
      "Iteration  128 => Loss: 138.644587\n",
      "Iteration  129 => Loss: 136.199747\n",
      "Iteration  130 => Loss: 133.798667\n",
      "Iteration  131 => Loss: 131.441347\n",
      "Iteration  132 => Loss: 129.127787\n",
      "Iteration  133 => Loss: 126.857987\n",
      "Iteration  134 => Loss: 124.631947\n",
      "Iteration  135 => Loss: 122.449667\n",
      "Iteration  136 => Loss: 120.311147\n",
      "Iteration  137 => Loss: 118.216387\n",
      "Iteration  138 => Loss: 116.165387\n",
      "Iteration  139 => Loss: 114.158147\n",
      "Iteration  140 => Loss: 112.194667\n",
      "Iteration  141 => Loss: 110.274947\n",
      "Iteration  142 => Loss: 108.398987\n",
      "Iteration  143 => Loss: 106.566787\n",
      "Iteration  144 => Loss: 104.778347\n",
      "Iteration  145 => Loss: 103.033667\n",
      "Iteration  146 => Loss: 101.332747\n",
      "Iteration  147 => Loss: 99.675587\n",
      "Iteration  148 => Loss: 98.062187\n",
      "Iteration  149 => Loss: 96.492547\n",
      "Iteration  150 => Loss: 94.966667\n",
      "Iteration  151 => Loss: 93.484547\n",
      "Iteration  152 => Loss: 92.046187\n",
      "Iteration  153 => Loss: 90.651587\n",
      "Iteration  154 => Loss: 89.300747\n",
      "Iteration  155 => Loss: 87.993667\n",
      "Iteration  156 => Loss: 86.730347\n",
      "Iteration  157 => Loss: 85.510787\n",
      "Iteration  158 => Loss: 84.334987\n",
      "Iteration  159 => Loss: 83.202947\n",
      "Iteration  160 => Loss: 82.114667\n",
      "Iteration  161 => Loss: 81.070147\n",
      "Iteration  162 => Loss: 80.069387\n",
      "Iteration  163 => Loss: 79.112387\n",
      "Iteration  164 => Loss: 78.199147\n",
      "Iteration  165 => Loss: 77.329667\n",
      "Iteration  166 => Loss: 76.503947\n",
      "Iteration  167 => Loss: 75.721987\n",
      "Iteration  168 => Loss: 74.983787\n",
      "Iteration  169 => Loss: 74.289347\n",
      "Iteration  170 => Loss: 73.638667\n",
      "Iteration  171 => Loss: 73.031747\n",
      "Iteration  172 => Loss: 72.468587\n",
      "Iteration  173 => Loss: 71.949187\n",
      "Iteration  174 => Loss: 71.473547\n",
      "Iteration  175 => Loss: 71.041667\n",
      "Iteration  176 => Loss: 70.653547\n",
      "Iteration  177 => Loss: 70.309187\n",
      "Iteration  178 => Loss: 70.008587\n",
      "Iteration  179 => Loss: 69.751747\n",
      "Iteration  180 => Loss: 69.538667\n",
      "Iteration  181 => Loss: 69.369347\n",
      "Iteration  182 => Loss: 69.243787\n",
      "Iteration  183 => Loss: 69.161987\n",
      "Iteration  184 => Loss: 69.123947\n",
      "Iteration  185 => Loss: 69.052847\n",
      "Iteration  186 => Loss: 68.981947\n",
      "Iteration  187 => Loss: 68.911247\n",
      "Iteration  188 => Loss: 68.840747\n",
      "Iteration  189 => Loss: 68.770447\n",
      "Iteration  190 => Loss: 68.700347\n",
      "Iteration  191 => Loss: 68.630447\n",
      "Iteration  192 => Loss: 68.560747\n",
      "Iteration  193 => Loss: 68.491247\n",
      "Iteration  194 => Loss: 68.421947\n",
      "Iteration  195 => Loss: 68.352847\n",
      "Iteration  196 => Loss: 68.283947\n",
      "Iteration  197 => Loss: 68.215247\n",
      "Iteration  198 => Loss: 68.146747\n",
      "Iteration  199 => Loss: 68.078447\n",
      "Iteration  200 => Loss: 68.010347\n",
      "Iteration  201 => Loss: 68.007853\n",
      "Iteration  202 => Loss: 67.937420\n",
      "Iteration  203 => Loss: 67.867187\n",
      "Iteration  204 => Loss: 67.797153\n",
      "Iteration  205 => Loss: 67.727320\n",
      "Iteration  206 => Loss: 67.657687\n",
      "Iteration  207 => Loss: 67.588253\n",
      "Iteration  208 => Loss: 67.519020\n",
      "Iteration  209 => Loss: 67.449987\n",
      "Iteration  210 => Loss: 67.381153\n",
      "Iteration  211 => Loss: 67.312520\n",
      "Iteration  212 => Loss: 67.244087\n",
      "Iteration  213 => Loss: 67.175853\n",
      "Iteration  214 => Loss: 67.107820\n",
      "Iteration  215 => Loss: 67.039987\n",
      "Iteration  216 => Loss: 66.972353\n",
      "Iteration  217 => Loss: 66.904920\n",
      "Iteration  218 => Loss: 66.837687\n",
      "Iteration  219 => Loss: 66.835887\n",
      "Iteration  220 => Loss: 66.766320\n",
      "Iteration  221 => Loss: 66.696953\n",
      "Iteration  222 => Loss: 66.627787\n",
      "Iteration  223 => Loss: 66.558820\n",
      "Iteration  224 => Loss: 66.490053\n",
      "Iteration  225 => Loss: 66.421487\n",
      "Iteration  226 => Loss: 66.353120\n",
      "Iteration  227 => Loss: 66.284953\n",
      "Iteration  228 => Loss: 66.216987\n",
      "Iteration  229 => Loss: 66.149220\n",
      "Iteration  230 => Loss: 66.081653\n",
      "Iteration  231 => Loss: 66.014287\n",
      "Iteration  232 => Loss: 65.947120\n",
      "Iteration  233 => Loss: 65.880153\n",
      "Iteration  234 => Loss: 65.813387\n",
      "Iteration  235 => Loss: 65.746820\n",
      "Iteration  236 => Loss: 65.680453\n",
      "Iteration  237 => Loss: 65.679347\n",
      "Iteration  238 => Loss: 65.610647\n",
      "Iteration  239 => Loss: 65.542147\n",
      "Iteration  240 => Loss: 65.473847\n",
      "Iteration  241 => Loss: 65.405747\n",
      "Iteration  242 => Loss: 65.337847\n",
      "Iteration  243 => Loss: 65.270147\n",
      "Iteration  244 => Loss: 65.202647\n",
      "Iteration  245 => Loss: 65.135347\n",
      "Iteration  246 => Loss: 65.068247\n",
      "Iteration  247 => Loss: 65.001347\n",
      "Iteration  248 => Loss: 64.934647\n",
      "Iteration  249 => Loss: 64.868147\n",
      "Iteration  250 => Loss: 64.801847\n",
      "Iteration  251 => Loss: 64.735747\n",
      "Iteration  252 => Loss: 64.669847\n",
      "Iteration  253 => Loss: 64.604147\n",
      "Iteration  254 => Loss: 64.538647\n",
      "Iteration  255 => Loss: 64.538233\n",
      "Iteration  256 => Loss: 64.470400\n",
      "Iteration  257 => Loss: 64.402767\n",
      "Iteration  258 => Loss: 64.335333\n",
      "Iteration  259 => Loss: 64.268100\n",
      "Iteration  260 => Loss: 64.201067\n",
      "Iteration  261 => Loss: 64.134233\n",
      "Iteration  262 => Loss: 64.067600\n",
      "Iteration  263 => Loss: 64.001167\n",
      "Iteration  264 => Loss: 63.934933\n",
      "Iteration  265 => Loss: 63.868900\n",
      "Iteration  266 => Loss: 63.803067\n",
      "Iteration  267 => Loss: 63.737433\n",
      "Iteration  268 => Loss: 63.672000\n",
      "Iteration  269 => Loss: 63.606767\n",
      "Iteration  270 => Loss: 63.541733\n",
      "Iteration  271 => Loss: 63.476900\n",
      "Iteration  272 => Loss: 63.412267\n",
      "Iteration  273 => Loss: 63.347833\n",
      "Iteration  274 => Loss: 63.345580\n",
      "Iteration  275 => Loss: 63.278813\n",
      "Iteration  276 => Loss: 63.212247\n",
      "Iteration  277 => Loss: 63.145880\n",
      "Iteration  278 => Loss: 63.079713\n",
      "Iteration  279 => Loss: 63.013747\n",
      "Iteration  280 => Loss: 62.947980\n",
      "Iteration  281 => Loss: 62.882413\n",
      "Iteration  282 => Loss: 62.817047\n",
      "Iteration  283 => Loss: 62.751880\n",
      "Iteration  284 => Loss: 62.686913\n",
      "Iteration  285 => Loss: 62.622147\n",
      "Iteration  286 => Loss: 62.557580\n",
      "Iteration  287 => Loss: 62.493213\n",
      "Iteration  288 => Loss: 62.429047\n",
      "Iteration  289 => Loss: 62.365080\n",
      "Iteration  290 => Loss: 62.301313\n",
      "Iteration  291 => Loss: 62.237747\n",
      "Iteration  292 => Loss: 62.236187\n",
      "Iteration  293 => Loss: 62.170287\n",
      "Iteration  294 => Loss: 62.104587\n",
      "Iteration  295 => Loss: 62.039087\n",
      "Iteration  296 => Loss: 61.973787\n",
      "Iteration  297 => Loss: 61.908687\n",
      "Iteration  298 => Loss: 61.843787\n",
      "Iteration  299 => Loss: 61.779087\n",
      "Iteration  300 => Loss: 61.714587\n",
      "Iteration  301 => Loss: 61.650287\n",
      "Iteration  302 => Loss: 61.586187\n",
      "Iteration  303 => Loss: 61.522287\n",
      "Iteration  304 => Loss: 61.458587\n",
      "Iteration  305 => Loss: 61.395087\n",
      "Iteration  306 => Loss: 61.331787\n",
      "Iteration  307 => Loss: 61.268687\n",
      "Iteration  308 => Loss: 61.205787\n",
      "Iteration  309 => Loss: 61.143087\n",
      "Iteration  310 => Loss: 61.142220\n",
      "Iteration  311 => Loss: 61.077187\n",
      "Iteration  312 => Loss: 61.012353\n",
      "Iteration  313 => Loss: 60.947720\n",
      "Iteration  314 => Loss: 60.883287\n",
      "Iteration  315 => Loss: 60.819053\n",
      "Iteration  316 => Loss: 60.755020\n",
      "Iteration  317 => Loss: 60.691187\n",
      "Iteration  318 => Loss: 60.627553\n",
      "Iteration  319 => Loss: 60.564120\n",
      "Iteration  320 => Loss: 60.500887\n",
      "Iteration  321 => Loss: 60.437853\n",
      "Iteration  322 => Loss: 60.375020\n",
      "Iteration  323 => Loss: 60.312387\n",
      "Iteration  324 => Loss: 60.249953\n",
      "Iteration  325 => Loss: 60.187720\n",
      "Iteration  326 => Loss: 60.125687\n",
      "Iteration  327 => Loss: 60.063853\n",
      "Iteration  328 => Loss: 60.063680\n",
      "Iteration  329 => Loss: 59.999513\n",
      "Iteration  330 => Loss: 59.935547\n",
      "Iteration  331 => Loss: 59.871780\n",
      "Iteration  332 => Loss: 59.808213\n",
      "Iteration  333 => Loss: 59.744847\n",
      "Iteration  334 => Loss: 59.681680\n",
      "Iteration  335 => Loss: 59.618713\n",
      "Iteration  336 => Loss: 59.555947\n",
      "Iteration  337 => Loss: 59.493380\n",
      "Iteration  338 => Loss: 59.431013\n",
      "Iteration  339 => Loss: 59.368847\n",
      "Iteration  340 => Loss: 59.306880\n",
      "Iteration  341 => Loss: 59.245113\n",
      "Iteration  342 => Loss: 59.183547\n",
      "Iteration  343 => Loss: 59.122180\n",
      "Iteration  344 => Loss: 59.061013\n",
      "Iteration  345 => Loss: 59.000047\n",
      "Iteration  346 => Loss: 58.939280\n",
      "Iteration  347 => Loss: 58.937267\n",
      "Iteration  348 => Loss: 58.874167\n",
      "Iteration  349 => Loss: 58.811267\n",
      "Iteration  350 => Loss: 58.748567\n",
      "Iteration  351 => Loss: 58.686067\n",
      "Iteration  352 => Loss: 58.623767\n",
      "Iteration  353 => Loss: 58.561667\n",
      "Iteration  354 => Loss: 58.499767\n",
      "Iteration  355 => Loss: 58.438067\n",
      "Iteration  356 => Loss: 58.376567\n",
      "Iteration  357 => Loss: 58.315267\n",
      "Iteration  358 => Loss: 58.254167\n",
      "Iteration  359 => Loss: 58.193267\n",
      "Iteration  360 => Loss: 58.132567\n",
      "Iteration  361 => Loss: 58.072067\n",
      "Iteration  362 => Loss: 58.011767\n",
      "Iteration  363 => Loss: 57.951667\n",
      "Iteration  364 => Loss: 57.891767\n",
      "Iteration  365 => Loss: 57.890447\n",
      "Iteration  366 => Loss: 57.828213\n",
      "Iteration  367 => Loss: 57.766180\n",
      "Iteration  368 => Loss: 57.704347\n",
      "Iteration  369 => Loss: 57.642713\n",
      "Iteration  370 => Loss: 57.581280\n",
      "Iteration  371 => Loss: 57.520047\n",
      "Iteration  372 => Loss: 57.459013\n",
      "Iteration  373 => Loss: 57.398180\n",
      "Iteration  374 => Loss: 57.337547\n",
      "Iteration  375 => Loss: 57.277113\n",
      "Iteration  376 => Loss: 57.216880\n",
      "Iteration  377 => Loss: 57.156847\n",
      "Iteration  378 => Loss: 57.097013\n",
      "Iteration  379 => Loss: 57.037380\n",
      "Iteration  380 => Loss: 56.977947\n",
      "Iteration  381 => Loss: 56.918713\n",
      "Iteration  382 => Loss: 56.859680\n",
      "Iteration  383 => Loss: 56.859053\n",
      "Iteration  384 => Loss: 56.797687\n",
      "Iteration  385 => Loss: 56.736520\n",
      "Iteration  386 => Loss: 56.675553\n",
      "Iteration  387 => Loss: 56.614787\n",
      "Iteration  388 => Loss: 56.554220\n",
      "Iteration  389 => Loss: 56.493853\n",
      "Iteration  390 => Loss: 56.433687\n",
      "Iteration  391 => Loss: 56.373720\n",
      "Iteration  392 => Loss: 56.313953\n",
      "Iteration  393 => Loss: 56.254387\n",
      "Iteration  394 => Loss: 56.195020\n",
      "Iteration  395 => Loss: 56.135853\n",
      "Iteration  396 => Loss: 56.076887\n",
      "Iteration  397 => Loss: 56.018120\n",
      "Iteration  398 => Loss: 55.959553\n",
      "Iteration  399 => Loss: 55.901187\n",
      "Iteration  400 => Loss: 55.843020\n",
      "Iteration  401 => Loss: 55.785053\n",
      "Iteration  402 => Loss: 55.782587\n",
      "Iteration  403 => Loss: 55.722287\n",
      "Iteration  404 => Loss: 55.662187\n",
      "Iteration  405 => Loss: 55.602287\n",
      "Iteration  406 => Loss: 55.542587\n",
      "Iteration  407 => Loss: 55.483087\n",
      "Iteration  408 => Loss: 55.423787\n",
      "Iteration  409 => Loss: 55.364687\n",
      "Iteration  410 => Loss: 55.305787\n",
      "Iteration  411 => Loss: 55.247087\n",
      "Iteration  412 => Loss: 55.188587\n",
      "Iteration  413 => Loss: 55.130287\n",
      "Iteration  414 => Loss: 55.072187\n",
      "Iteration  415 => Loss: 55.014287\n",
      "Iteration  416 => Loss: 54.956587\n",
      "Iteration  417 => Loss: 54.899087\n",
      "Iteration  418 => Loss: 54.841787\n",
      "Iteration  419 => Loss: 54.784687\n",
      "Iteration  420 => Loss: 54.782913\n",
      "Iteration  421 => Loss: 54.723480\n",
      "Iteration  422 => Loss: 54.664247\n",
      "Iteration  423 => Loss: 54.605213\n",
      "Iteration  424 => Loss: 54.546380\n",
      "Iteration  425 => Loss: 54.487747\n",
      "Iteration  426 => Loss: 54.429313\n",
      "Iteration  427 => Loss: 54.371080\n",
      "Iteration  428 => Loss: 54.313047\n",
      "Iteration  429 => Loss: 54.255213\n",
      "Iteration  430 => Loss: 54.197580\n",
      "Iteration  431 => Loss: 54.140147\n",
      "Iteration  432 => Loss: 54.082913\n",
      "Iteration  433 => Loss: 54.025880\n",
      "Iteration  434 => Loss: 53.969047\n",
      "Iteration  435 => Loss: 53.912413\n",
      "Iteration  436 => Loss: 53.855980\n",
      "Iteration  437 => Loss: 53.799747\n",
      "Iteration  438 => Loss: 53.798667\n",
      "Iteration  439 => Loss: 53.740100\n",
      "Iteration  440 => Loss: 53.681733\n",
      "Iteration  441 => Loss: 53.623567\n",
      "Iteration  442 => Loss: 53.565600\n",
      "Iteration  443 => Loss: 53.507833\n",
      "Iteration  444 => Loss: 53.450267\n",
      "Iteration  445 => Loss: 53.392900\n",
      "Iteration  446 => Loss: 53.335733\n",
      "Iteration  447 => Loss: 53.278767\n",
      "Iteration  448 => Loss: 53.222000\n",
      "Iteration  449 => Loss: 53.165433\n",
      "Iteration  450 => Loss: 53.109067\n",
      "Iteration  451 => Loss: 53.052900\n",
      "Iteration  452 => Loss: 52.996933\n",
      "Iteration  453 => Loss: 52.941167\n",
      "Iteration  454 => Loss: 52.885600\n",
      "Iteration  455 => Loss: 52.830233\n",
      "Iteration  456 => Loss: 52.829847\n",
      "Iteration  457 => Loss: 52.772147\n",
      "Iteration  458 => Loss: 52.714647\n",
      "Iteration  459 => Loss: 52.657347\n",
      "Iteration  460 => Loss: 52.600247\n",
      "Iteration  461 => Loss: 52.543347\n",
      "Iteration  462 => Loss: 52.486647\n",
      "Iteration  463 => Loss: 52.430147\n",
      "Iteration  464 => Loss: 52.373847\n",
      "Iteration  465 => Loss: 52.317747\n",
      "Iteration  466 => Loss: 52.261847\n",
      "Iteration  467 => Loss: 52.206147\n",
      "Iteration  468 => Loss: 52.150647\n",
      "Iteration  469 => Loss: 52.095347\n",
      "Iteration  470 => Loss: 52.040247\n",
      "Iteration  471 => Loss: 51.985347\n",
      "Iteration  472 => Loss: 51.930647\n",
      "Iteration  473 => Loss: 51.876147\n",
      "Iteration  474 => Loss: 51.821847\n",
      "Iteration  475 => Loss: 51.819620\n",
      "Iteration  476 => Loss: 51.762987\n",
      "Iteration  477 => Loss: 51.706553\n",
      "Iteration  478 => Loss: 51.650320\n",
      "Iteration  479 => Loss: 51.594287\n",
      "Iteration  480 => Loss: 51.538453\n",
      "Iteration  481 => Loss: 51.482820\n",
      "Iteration  482 => Loss: 51.427387\n",
      "Iteration  483 => Loss: 51.372153\n",
      "Iteration  484 => Loss: 51.317120\n",
      "Iteration  485 => Loss: 51.262287\n",
      "Iteration  486 => Loss: 51.207653\n",
      "Iteration  487 => Loss: 51.153220\n",
      "Iteration  488 => Loss: 51.098987\n",
      "Iteration  489 => Loss: 51.044953\n",
      "Iteration  490 => Loss: 50.991120\n",
      "Iteration  491 => Loss: 50.937487\n",
      "Iteration  492 => Loss: 50.884053\n",
      "Iteration  493 => Loss: 50.882520\n",
      "Iteration  494 => Loss: 50.826753\n",
      "Iteration  495 => Loss: 50.771187\n",
      "Iteration  496 => Loss: 50.715820\n",
      "Iteration  497 => Loss: 50.660653\n",
      "Iteration  498 => Loss: 50.605687\n",
      "Iteration  499 => Loss: 50.550920\n",
      "Iteration  500 => Loss: 50.496353\n",
      "Iteration  501 => Loss: 50.441987\n",
      "Iteration  502 => Loss: 50.387820\n",
      "Iteration  503 => Loss: 50.333853\n",
      "Iteration  504 => Loss: 50.280087\n",
      "Iteration  505 => Loss: 50.226520\n",
      "Iteration  506 => Loss: 50.173153\n",
      "Iteration  507 => Loss: 50.119987\n",
      "Iteration  508 => Loss: 50.067020\n",
      "Iteration  509 => Loss: 50.014253\n",
      "Iteration  510 => Loss: 49.961687\n",
      "Iteration  511 => Loss: 49.960847\n",
      "Iteration  512 => Loss: 49.905947\n",
      "Iteration  513 => Loss: 49.851247\n",
      "Iteration  514 => Loss: 49.796747\n",
      "Iteration  515 => Loss: 49.742447\n",
      "Iteration  516 => Loss: 49.688347\n",
      "Iteration  517 => Loss: 49.634447\n",
      "Iteration  518 => Loss: 49.580747\n",
      "Iteration  519 => Loss: 49.527247\n",
      "Iteration  520 => Loss: 49.473947\n",
      "Iteration  521 => Loss: 49.420847\n",
      "Iteration  522 => Loss: 49.367947\n",
      "Iteration  523 => Loss: 49.315247\n",
      "Iteration  524 => Loss: 49.262747\n",
      "Iteration  525 => Loss: 49.210447\n",
      "Iteration  526 => Loss: 49.158347\n",
      "Iteration  527 => Loss: 49.106447\n",
      "Iteration  528 => Loss: 49.054747\n",
      "Iteration  529 => Loss: 49.054600\n",
      "Iteration  530 => Loss: 49.000567\n",
      "Iteration  531 => Loss: 48.946733\n",
      "Iteration  532 => Loss: 48.893100\n",
      "Iteration  533 => Loss: 48.839667\n",
      "Iteration  534 => Loss: 48.786433\n",
      "Iteration  535 => Loss: 48.733400\n",
      "Iteration  536 => Loss: 48.680567\n",
      "Iteration  537 => Loss: 48.627933\n",
      "Iteration  538 => Loss: 48.575500\n",
      "Iteration  539 => Loss: 48.523267\n",
      "Iteration  540 => Loss: 48.471233\n",
      "Iteration  541 => Loss: 48.419400\n",
      "Iteration  542 => Loss: 48.367767\n",
      "Iteration  543 => Loss: 48.316333\n",
      "Iteration  544 => Loss: 48.265100\n",
      "Iteration  545 => Loss: 48.214067\n",
      "Iteration  546 => Loss: 48.163233\n",
      "Iteration  547 => Loss: 48.112600\n",
      "Iteration  548 => Loss: 48.110613\n",
      "Iteration  549 => Loss: 48.057647\n",
      "Iteration  550 => Loss: 48.004880\n",
      "Iteration  551 => Loss: 47.952313\n",
      "Iteration  552 => Loss: 47.899947\n",
      "Iteration  553 => Loss: 47.847780\n",
      "Iteration  554 => Loss: 47.795813\n",
      "Iteration  555 => Loss: 47.744047\n",
      "Iteration  556 => Loss: 47.692480\n",
      "Iteration  557 => Loss: 47.641113\n",
      "Iteration  558 => Loss: 47.589947\n",
      "Iteration  559 => Loss: 47.538980\n",
      "Iteration  560 => Loss: 47.488213\n",
      "Iteration  561 => Loss: 47.437647\n",
      "Iteration  562 => Loss: 47.387280\n",
      "Iteration  563 => Loss: 47.337113\n",
      "Iteration  564 => Loss: 47.287147\n",
      "Iteration  565 => Loss: 47.237380\n",
      "Iteration  566 => Loss: 47.236087\n",
      "Iteration  567 => Loss: 47.183987\n",
      "Iteration  568 => Loss: 47.132087\n",
      "Iteration  569 => Loss: 47.080387\n",
      "Iteration  570 => Loss: 47.028887\n",
      "Iteration  571 => Loss: 46.977587\n",
      "Iteration  572 => Loss: 46.926487\n",
      "Iteration  573 => Loss: 46.875587\n",
      "Iteration  574 => Loss: 46.824887\n",
      "Iteration  575 => Loss: 46.774387\n",
      "Iteration  576 => Loss: 46.724087\n",
      "Iteration  577 => Loss: 46.673987\n",
      "Iteration  578 => Loss: 46.624087\n",
      "Iteration  579 => Loss: 46.574387\n",
      "Iteration  580 => Loss: 46.524887\n",
      "Iteration  581 => Loss: 46.475587\n",
      "Iteration  582 => Loss: 46.426487\n",
      "Iteration  583 => Loss: 46.377587\n",
      "Iteration  584 => Loss: 46.376987\n",
      "Iteration  585 => Loss: 46.325753\n",
      "Iteration  586 => Loss: 46.274720\n",
      "Iteration  587 => Loss: 46.223887\n",
      "Iteration  588 => Loss: 46.173253\n",
      "Iteration  589 => Loss: 46.122820\n",
      "Iteration  590 => Loss: 46.072587\n",
      "Iteration  591 => Loss: 46.022553\n",
      "Iteration  592 => Loss: 45.972720\n",
      "Iteration  593 => Loss: 45.923087\n",
      "Iteration  594 => Loss: 45.873653\n",
      "Iteration  595 => Loss: 45.824420\n",
      "Iteration  596 => Loss: 45.775387\n",
      "Iteration  597 => Loss: 45.726553\n",
      "Iteration  598 => Loss: 45.677920\n",
      "Iteration  599 => Loss: 45.629487\n",
      "Iteration  600 => Loss: 45.581253\n",
      "Iteration  601 => Loss: 45.533220\n",
      "Iteration  602 => Loss: 45.485387\n",
      "Iteration  603 => Loss: 45.482947\n",
      "Iteration  604 => Loss: 45.432780\n",
      "Iteration  605 => Loss: 45.382813\n",
      "Iteration  606 => Loss: 45.333047\n",
      "Iteration  607 => Loss: 45.283480\n",
      "Iteration  608 => Loss: 45.234113\n",
      "Iteration  609 => Loss: 45.184947\n",
      "Iteration  610 => Loss: 45.135980\n",
      "Iteration  611 => Loss: 45.087213\n",
      "Iteration  612 => Loss: 45.038647\n",
      "Iteration  613 => Loss: 44.990280\n",
      "Iteration  614 => Loss: 44.942113\n",
      "Iteration  615 => Loss: 44.894147\n",
      "Iteration  616 => Loss: 44.846380\n",
      "Iteration  617 => Loss: 44.798813\n",
      "Iteration  618 => Loss: 44.751447\n",
      "Iteration  619 => Loss: 44.704280\n",
      "Iteration  620 => Loss: 44.657313\n",
      "Iteration  621 => Loss: 44.655567\n",
      "Iteration  622 => Loss: 44.606267\n",
      "Iteration  623 => Loss: 44.557167\n",
      "Iteration  624 => Loss: 44.508267\n",
      "Iteration  625 => Loss: 44.459567\n",
      "Iteration  626 => Loss: 44.411067\n",
      "Iteration  627 => Loss: 44.362767\n",
      "Iteration  628 => Loss: 44.314667\n",
      "Iteration  629 => Loss: 44.266767\n",
      "Iteration  630 => Loss: 44.219067\n",
      "Iteration  631 => Loss: 44.171567\n",
      "Iteration  632 => Loss: 44.124267\n",
      "Iteration  633 => Loss: 44.077167\n",
      "Iteration  634 => Loss: 44.030267\n",
      "Iteration  635 => Loss: 43.983567\n",
      "Iteration  636 => Loss: 43.937067\n",
      "Iteration  637 => Loss: 43.890767\n",
      "Iteration  638 => Loss: 43.844667\n",
      "Iteration  639 => Loss: 43.843613\n",
      "Iteration  640 => Loss: 43.795180\n",
      "Iteration  641 => Loss: 43.746947\n",
      "Iteration  642 => Loss: 43.698913\n",
      "Iteration  643 => Loss: 43.651080\n",
      "Iteration  644 => Loss: 43.603447\n",
      "Iteration  645 => Loss: 43.556013\n",
      "Iteration  646 => Loss: 43.508780\n",
      "Iteration  647 => Loss: 43.461747\n",
      "Iteration  648 => Loss: 43.414913\n",
      "Iteration  649 => Loss: 43.368280\n",
      "Iteration  650 => Loss: 43.321847\n",
      "Iteration  651 => Loss: 43.275613\n",
      "Iteration  652 => Loss: 43.229580\n",
      "Iteration  653 => Loss: 43.183747\n",
      "Iteration  654 => Loss: 43.138113\n",
      "Iteration  655 => Loss: 43.092680\n",
      "Iteration  656 => Loss: 43.047447\n",
      "Iteration  657 => Loss: 43.047087\n",
      "Iteration  658 => Loss: 42.999520\n",
      "Iteration  659 => Loss: 42.952153\n",
      "Iteration  660 => Loss: 42.904987\n",
      "Iteration  661 => Loss: 42.858020\n",
      "Iteration  662 => Loss: 42.811253\n",
      "Iteration  663 => Loss: 42.764687\n",
      "Iteration  664 => Loss: 42.718320\n",
      "Iteration  665 => Loss: 42.672153\n",
      "Iteration  666 => Loss: 42.626187\n",
      "Iteration  667 => Loss: 42.580420\n",
      "Iteration  668 => Loss: 42.534853\n",
      "Iteration  669 => Loss: 42.489487\n",
      "Iteration  670 => Loss: 42.444320\n",
      "Iteration  671 => Loss: 42.399353\n",
      "Iteration  672 => Loss: 42.354587\n",
      "Iteration  673 => Loss: 42.310020\n",
      "Iteration  674 => Loss: 42.265653\n",
      "Iteration  675 => Loss: 42.221487\n",
      "Iteration  676 => Loss: 42.219287\n",
      "Iteration  677 => Loss: 42.172787\n",
      "Iteration  678 => Loss: 42.126487\n",
      "Iteration  679 => Loss: 42.080387\n",
      "Iteration  680 => Loss: 42.034487\n",
      "Iteration  681 => Loss: 41.988787\n",
      "Iteration  682 => Loss: 41.943287\n",
      "Iteration  683 => Loss: 41.897987\n",
      "Iteration  684 => Loss: 41.852887\n",
      "Iteration  685 => Loss: 41.807987\n",
      "Iteration  686 => Loss: 41.763287\n",
      "Iteration  687 => Loss: 41.718787\n",
      "Iteration  688 => Loss: 41.674487\n",
      "Iteration  689 => Loss: 41.630387\n",
      "Iteration  690 => Loss: 41.586487\n",
      "Iteration  691 => Loss: 41.542787\n",
      "Iteration  692 => Loss: 41.499287\n",
      "Iteration  693 => Loss: 41.455987\n",
      "Iteration  694 => Loss: 41.454480\n",
      "Iteration  695 => Loss: 41.408847\n",
      "Iteration  696 => Loss: 41.363413\n",
      "Iteration  697 => Loss: 41.318180\n",
      "Iteration  698 => Loss: 41.273147\n",
      "Iteration  699 => Loss: 41.228313\n",
      "Iteration  700 => Loss: 41.183680\n",
      "Iteration  701 => Loss: 41.139247\n",
      "Iteration  702 => Loss: 41.095013\n",
      "Iteration  703 => Loss: 41.050980\n",
      "Iteration  704 => Loss: 41.007147\n",
      "Iteration  705 => Loss: 40.963513\n",
      "Iteration  706 => Loss: 40.920080\n",
      "Iteration  707 => Loss: 40.876847\n",
      "Iteration  708 => Loss: 40.833813\n",
      "Iteration  709 => Loss: 40.790980\n",
      "Iteration  710 => Loss: 40.748347\n",
      "Iteration  711 => Loss: 40.705913\n",
      "Iteration  712 => Loss: 40.705100\n",
      "Iteration  713 => Loss: 40.660333\n",
      "Iteration  714 => Loss: 40.615767\n",
      "Iteration  715 => Loss: 40.571400\n",
      "Iteration  716 => Loss: 40.527233\n",
      "Iteration  717 => Loss: 40.483267\n",
      "Iteration  718 => Loss: 40.439500\n",
      "Iteration  719 => Loss: 40.395933\n",
      "Iteration  720 => Loss: 40.352567\n",
      "Iteration  721 => Loss: 40.309400\n",
      "Iteration  722 => Loss: 40.266433\n",
      "Iteration  723 => Loss: 40.223667\n",
      "Iteration  724 => Loss: 40.181100\n",
      "Iteration  725 => Loss: 40.138733\n",
      "Iteration  726 => Loss: 40.096567\n",
      "Iteration  727 => Loss: 40.054600\n",
      "Iteration  728 => Loss: 40.012833\n",
      "Iteration  729 => Loss: 39.971267\n",
      "Iteration  730 => Loss: 39.971147\n",
      "Iteration  731 => Loss: 39.927247\n",
      "Iteration  732 => Loss: 39.883547\n",
      "Iteration  733 => Loss: 39.840047\n",
      "Iteration  734 => Loss: 39.796747\n",
      "Iteration  735 => Loss: 39.753647\n",
      "Iteration  736 => Loss: 39.710747\n",
      "Iteration  737 => Loss: 39.668047\n",
      "Iteration  738 => Loss: 39.625547\n",
      "Iteration  739 => Loss: 39.583247\n",
      "Iteration  740 => Loss: 39.541147\n",
      "Iteration  741 => Loss: 39.499247\n",
      "Iteration  742 => Loss: 39.457547\n",
      "Iteration  743 => Loss: 39.416047\n",
      "Iteration  744 => Loss: 39.374747\n",
      "Iteration  745 => Loss: 39.333647\n",
      "Iteration  746 => Loss: 39.292747\n",
      "Iteration  747 => Loss: 39.252047\n",
      "Iteration  748 => Loss: 39.211547\n",
      "Iteration  749 => Loss: 39.209587\n",
      "Iteration  750 => Loss: 39.166753\n",
      "Iteration  751 => Loss: 39.124120\n",
      "Iteration  752 => Loss: 39.081687\n",
      "Iteration  753 => Loss: 39.039453\n",
      "Iteration  754 => Loss: 38.997420\n",
      "Iteration  755 => Loss: 38.955587\n",
      "Iteration  756 => Loss: 38.913953\n",
      "Iteration  757 => Loss: 38.872520\n",
      "Iteration  758 => Loss: 38.831287\n",
      "Iteration  759 => Loss: 38.790253\n",
      "Iteration  760 => Loss: 38.749420\n",
      "Iteration  761 => Loss: 38.708787\n",
      "Iteration  762 => Loss: 38.668353\n",
      "Iteration  763 => Loss: 38.628120\n",
      "Iteration  764 => Loss: 38.588087\n",
      "Iteration  765 => Loss: 38.548253\n",
      "Iteration  766 => Loss: 38.508620\n",
      "Iteration  767 => Loss: 38.507353\n",
      "Iteration  768 => Loss: 38.465387\n",
      "Iteration  769 => Loss: 38.423620\n",
      "Iteration  770 => Loss: 38.382053\n",
      "Iteration  771 => Loss: 38.340687\n",
      "Iteration  772 => Loss: 38.299520\n",
      "Iteration  773 => Loss: 38.258553\n",
      "Iteration  774 => Loss: 38.217787\n",
      "Iteration  775 => Loss: 38.177220\n",
      "Iteration  776 => Loss: 38.136853\n",
      "Iteration  777 => Loss: 38.096687\n",
      "Iteration  778 => Loss: 38.056720\n",
      "Iteration  779 => Loss: 38.016953\n",
      "Iteration  780 => Loss: 37.977387\n",
      "Iteration  781 => Loss: 37.938020\n",
      "Iteration  782 => Loss: 37.898853\n",
      "Iteration  783 => Loss: 37.859887\n",
      "Iteration  784 => Loss: 37.821120\n",
      "Iteration  785 => Loss: 37.820547\n",
      "Iteration  786 => Loss: 37.779447\n",
      "Iteration  787 => Loss: 37.738547\n",
      "Iteration  788 => Loss: 37.697847\n",
      "Iteration  789 => Loss: 37.657347\n",
      "Iteration  790 => Loss: 37.617047\n",
      "Iteration  791 => Loss: 37.576947\n",
      "Iteration  792 => Loss: 37.537047\n",
      "Iteration  793 => Loss: 37.497347\n",
      "Iteration  794 => Loss: 37.457847\n",
      "Iteration  795 => Loss: 37.418547\n",
      "Iteration  796 => Loss: 37.379447\n",
      "Iteration  797 => Loss: 37.340547\n",
      "Iteration  798 => Loss: 37.301847\n",
      "Iteration  799 => Loss: 37.263347\n",
      "Iteration  800 => Loss: 37.225047\n",
      "Iteration  801 => Loss: 37.186947\n",
      "Iteration  802 => Loss: 37.149047\n",
      "Iteration  803 => Loss: 37.111347\n",
      "Iteration  804 => Loss: 37.108933\n",
      "Iteration  805 => Loss: 37.068900\n",
      "Iteration  806 => Loss: 37.029067\n",
      "Iteration  807 => Loss: 36.989433\n",
      "Iteration  808 => Loss: 36.950000\n",
      "Iteration  809 => Loss: 36.910767\n",
      "Iteration  810 => Loss: 36.871733\n",
      "Iteration  811 => Loss: 36.832900\n",
      "Iteration  812 => Loss: 36.794267\n",
      "Iteration  813 => Loss: 36.755833\n",
      "Iteration  814 => Loss: 36.717600\n",
      "Iteration  815 => Loss: 36.679567\n",
      "Iteration  816 => Loss: 36.641733\n",
      "Iteration  817 => Loss: 36.604100\n",
      "Iteration  818 => Loss: 36.566667\n",
      "Iteration  819 => Loss: 36.529433\n",
      "Iteration  820 => Loss: 36.492400\n",
      "Iteration  821 => Loss: 36.455567\n",
      "Iteration  822 => Loss: 36.453847\n",
      "Iteration  823 => Loss: 36.414680\n",
      "Iteration  824 => Loss: 36.375713\n",
      "Iteration  825 => Loss: 36.336947\n",
      "Iteration  826 => Loss: 36.298380\n",
      "Iteration  827 => Loss: 36.260013\n",
      "Iteration  828 => Loss: 36.221847\n",
      "Iteration  829 => Loss: 36.183880\n",
      "Iteration  830 => Loss: 36.146113\n",
      "Iteration  831 => Loss: 36.108547\n",
      "Iteration  832 => Loss: 36.071180\n",
      "Iteration  833 => Loss: 36.034013\n",
      "Iteration  834 => Loss: 35.997047\n",
      "Iteration  835 => Loss: 35.960280\n",
      "Iteration  836 => Loss: 35.923713\n",
      "Iteration  837 => Loss: 35.887347\n",
      "Iteration  838 => Loss: 35.851180\n",
      "Iteration  839 => Loss: 35.815213\n",
      "Iteration  840 => Loss: 35.814187\n",
      "Iteration  841 => Loss: 35.775887\n",
      "Iteration  842 => Loss: 35.737787\n",
      "Iteration  843 => Loss: 35.699887\n",
      "Iteration  844 => Loss: 35.662187\n",
      "Iteration  845 => Loss: 35.624687\n",
      "Iteration  846 => Loss: 35.587387\n",
      "Iteration  847 => Loss: 35.550287\n",
      "Iteration  848 => Loss: 35.513387\n",
      "Iteration  849 => Loss: 35.476687\n",
      "Iteration  850 => Loss: 35.440187\n",
      "Iteration  851 => Loss: 35.403887\n",
      "Iteration  852 => Loss: 35.367787\n",
      "Iteration  853 => Loss: 35.331887\n",
      "Iteration  854 => Loss: 35.296187\n",
      "Iteration  855 => Loss: 35.260687\n",
      "Iteration  856 => Loss: 35.225387\n",
      "Iteration  857 => Loss: 35.190287\n",
      "Iteration  858 => Loss: 35.189953\n",
      "Iteration  859 => Loss: 35.152520\n",
      "Iteration  860 => Loss: 35.115287\n",
      "Iteration  861 => Loss: 35.078253\n",
      "Iteration  862 => Loss: 35.041420\n",
      "Iteration  863 => Loss: 35.004787\n",
      "Iteration  864 => Loss: 34.968353\n",
      "Iteration  865 => Loss: 34.932120\n",
      "Iteration  866 => Loss: 34.896087\n",
      "Iteration  867 => Loss: 34.860253\n",
      "Iteration  868 => Loss: 34.824620\n",
      "Iteration  869 => Loss: 34.789187\n",
      "Iteration  870 => Loss: 34.753953\n",
      "Iteration  871 => Loss: 34.718920\n",
      "Iteration  872 => Loss: 34.684087\n",
      "Iteration  873 => Loss: 34.649453\n",
      "Iteration  874 => Loss: 34.615020\n",
      "Iteration  875 => Loss: 34.580787\n",
      "Iteration  876 => Loss: 34.546753\n",
      "Iteration  877 => Loss: 34.544580\n",
      "Iteration  878 => Loss: 34.508213\n",
      "Iteration  879 => Loss: 34.472047\n",
      "Iteration  880 => Loss: 34.436080\n",
      "Iteration  881 => Loss: 34.400313\n",
      "Iteration  882 => Loss: 34.364747\n",
      "Iteration  883 => Loss: 34.329380\n",
      "Iteration  884 => Loss: 34.294213\n",
      "Iteration  885 => Loss: 34.259247\n",
      "Iteration  886 => Loss: 34.224480\n",
      "Iteration  887 => Loss: 34.189913\n",
      "Iteration  888 => Loss: 34.155547\n",
      "Iteration  889 => Loss: 34.121380\n",
      "Iteration  890 => Loss: 34.087413\n",
      "Iteration  891 => Loss: 34.053647\n",
      "Iteration  892 => Loss: 34.020080\n",
      "Iteration  893 => Loss: 33.986713\n",
      "Iteration  894 => Loss: 33.953547\n",
      "Iteration  895 => Loss: 33.952067\n",
      "Iteration  896 => Loss: 33.916567\n",
      "Iteration  897 => Loss: 33.881267\n",
      "Iteration  898 => Loss: 33.846167\n",
      "Iteration  899 => Loss: 33.811267\n",
      "Iteration  900 => Loss: 33.776567\n",
      "Iteration  901 => Loss: 33.742067\n",
      "Iteration  902 => Loss: 33.707767\n",
      "Iteration  903 => Loss: 33.673667\n",
      "Iteration  904 => Loss: 33.639767\n",
      "Iteration  905 => Loss: 33.606067\n",
      "Iteration  906 => Loss: 33.572567\n",
      "Iteration  907 => Loss: 33.539267\n",
      "Iteration  908 => Loss: 33.506167\n",
      "Iteration  909 => Loss: 33.473267\n",
      "Iteration  910 => Loss: 33.440567\n",
      "Iteration  911 => Loss: 33.408067\n",
      "Iteration  912 => Loss: 33.375767\n",
      "Iteration  913 => Loss: 33.374980\n",
      "Iteration  914 => Loss: 33.340347\n",
      "Iteration  915 => Loss: 33.305913\n",
      "Iteration  916 => Loss: 33.271680\n",
      "Iteration  917 => Loss: 33.237647\n",
      "Iteration  918 => Loss: 33.203813\n",
      "Iteration  919 => Loss: 33.170180\n",
      "Iteration  920 => Loss: 33.136747\n",
      "Iteration  921 => Loss: 33.103513\n",
      "Iteration  922 => Loss: 33.070480\n",
      "Iteration  923 => Loss: 33.037647\n",
      "Iteration  924 => Loss: 33.005013\n",
      "Iteration  925 => Loss: 32.972580\n",
      "Iteration  926 => Loss: 32.940347\n",
      "Iteration  927 => Loss: 32.908313\n",
      "Iteration  928 => Loss: 32.876480\n",
      "Iteration  929 => Loss: 32.844847\n",
      "Iteration  930 => Loss: 32.813413\n",
      "Iteration  931 => Loss: 32.813320\n",
      "Iteration  932 => Loss: 32.779553\n",
      "Iteration  933 => Loss: 32.745987\n",
      "Iteration  934 => Loss: 32.712620\n",
      "Iteration  935 => Loss: 32.679453\n",
      "Iteration  936 => Loss: 32.646487\n",
      "Iteration  937 => Loss: 32.613720\n",
      "Iteration  938 => Loss: 32.581153\n",
      "Iteration  939 => Loss: 32.548787\n",
      "Iteration  940 => Loss: 32.516620\n",
      "Iteration  941 => Loss: 32.484653\n",
      "Iteration  942 => Loss: 32.452887\n",
      "Iteration  943 => Loss: 32.421320\n",
      "Iteration  944 => Loss: 32.389953\n",
      "Iteration  945 => Loss: 32.358787\n",
      "Iteration  946 => Loss: 32.327820\n",
      "Iteration  947 => Loss: 32.297053\n",
      "Iteration  948 => Loss: 32.266487\n",
      "Iteration  949 => Loss: 32.236120\n",
      "Iteration  950 => Loss: 32.234187\n",
      "Iteration  951 => Loss: 32.201487\n",
      "Iteration  952 => Loss: 32.168987\n",
      "Iteration  953 => Loss: 32.136687\n",
      "Iteration  954 => Loss: 32.104587\n",
      "Iteration  955 => Loss: 32.072687\n",
      "Iteration  956 => Loss: 32.040987\n",
      "Iteration  957 => Loss: 32.009487\n",
      "Iteration  958 => Loss: 31.978187\n",
      "Iteration  959 => Loss: 31.947087\n",
      "Iteration  960 => Loss: 31.916187\n",
      "Iteration  961 => Loss: 31.885487\n",
      "Iteration  962 => Loss: 31.854987\n",
      "Iteration  963 => Loss: 31.824687\n",
      "Iteration  964 => Loss: 31.794587\n",
      "Iteration  965 => Loss: 31.764687\n",
      "Iteration  966 => Loss: 31.734987\n",
      "Iteration  967 => Loss: 31.705487\n",
      "Iteration  968 => Loss: 31.704247\n",
      "Iteration  969 => Loss: 31.672413\n",
      "Iteration  970 => Loss: 31.640780\n",
      "Iteration  971 => Loss: 31.609347\n",
      "Iteration  972 => Loss: 31.578113\n",
      "Iteration  973 => Loss: 31.547080\n",
      "Iteration  974 => Loss: 31.516247\n",
      "Iteration  975 => Loss: 31.485613\n",
      "Iteration  976 => Loss: 31.455180\n",
      "Iteration  977 => Loss: 31.424947\n",
      "Iteration  978 => Loss: 31.394913\n",
      "Iteration  979 => Loss: 31.365080\n",
      "Iteration  980 => Loss: 31.335447\n",
      "Iteration  981 => Loss: 31.306013\n",
      "Iteration  982 => Loss: 31.276780\n",
      "Iteration  983 => Loss: 31.247747\n",
      "Iteration  984 => Loss: 31.218913\n",
      "Iteration  985 => Loss: 31.190280\n",
      "Iteration  986 => Loss: 31.189733\n",
      "Iteration  987 => Loss: 31.158767\n",
      "Iteration  988 => Loss: 31.128000\n",
      "Iteration  989 => Loss: 31.097433\n",
      "Iteration  990 => Loss: 31.067067\n",
      "Iteration  991 => Loss: 31.036900\n",
      "Iteration  992 => Loss: 31.006933\n",
      "Iteration  993 => Loss: 30.977167\n",
      "Iteration  994 => Loss: 30.947600\n",
      "Iteration  995 => Loss: 30.918233\n",
      "Iteration  996 => Loss: 30.889067\n",
      "Iteration  997 => Loss: 30.860100\n",
      "Iteration  998 => Loss: 30.831333\n",
      "Iteration  999 => Loss: 30.802767\n",
      "Iteration 1000 => Loss: 30.774400\n",
      "Iteration 1001 => Loss: 30.746233\n",
      "Iteration 1002 => Loss: 30.718267\n",
      "Iteration 1003 => Loss: 30.690500\n",
      "Iteration 1004 => Loss: 30.662933\n",
      "Iteration 1005 => Loss: 30.660547\n",
      "Iteration 1006 => Loss: 30.630647\n",
      "Iteration 1007 => Loss: 30.600947\n",
      "Iteration 1008 => Loss: 30.571447\n",
      "Iteration 1009 => Loss: 30.542147\n",
      "Iteration 1010 => Loss: 30.513047\n",
      "Iteration 1011 => Loss: 30.484147\n",
      "Iteration 1012 => Loss: 30.455447\n",
      "Iteration 1013 => Loss: 30.426947\n",
      "Iteration 1014 => Loss: 30.398647\n",
      "Iteration 1015 => Loss: 30.370547\n",
      "Iteration 1016 => Loss: 30.342647\n",
      "Iteration 1017 => Loss: 30.314947\n",
      "Iteration 1018 => Loss: 30.287447\n",
      "Iteration 1019 => Loss: 30.260147\n",
      "Iteration 1020 => Loss: 30.233047\n",
      "Iteration 1021 => Loss: 30.206147\n",
      "Iteration 1022 => Loss: 30.179447\n",
      "Iteration 1023 => Loss: 30.177753\n",
      "Iteration 1024 => Loss: 30.148720\n",
      "Iteration 1025 => Loss: 30.119887\n",
      "Iteration 1026 => Loss: 30.091253\n",
      "Iteration 1027 => Loss: 30.062820\n",
      "Iteration 1028 => Loss: 30.034587\n",
      "Iteration 1029 => Loss: 30.006553\n",
      "Iteration 1030 => Loss: 29.978720\n",
      "Iteration 1031 => Loss: 29.951087\n",
      "Iteration 1032 => Loss: 29.923653\n",
      "Iteration 1033 => Loss: 29.896420\n",
      "Iteration 1034 => Loss: 29.869387\n",
      "Iteration 1035 => Loss: 29.842553\n",
      "Iteration 1036 => Loss: 29.815920\n",
      "Iteration 1037 => Loss: 29.789487\n",
      "Iteration 1038 => Loss: 29.763253\n",
      "Iteration 1039 => Loss: 29.737220\n",
      "Iteration 1040 => Loss: 29.711387\n",
      "Iteration 1041 => Loss: 29.710387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1042 => Loss: 29.682220\n",
      "Iteration 1043 => Loss: 29.654253\n",
      "Iteration 1044 => Loss: 29.626487\n",
      "Iteration 1045 => Loss: 29.598920\n",
      "Iteration 1046 => Loss: 29.571553\n",
      "Iteration 1047 => Loss: 29.544387\n",
      "Iteration 1048 => Loss: 29.517420\n",
      "Iteration 1049 => Loss: 29.490653\n",
      "Iteration 1050 => Loss: 29.464087\n",
      "Iteration 1051 => Loss: 29.437720\n",
      "Iteration 1052 => Loss: 29.411553\n",
      "Iteration 1053 => Loss: 29.385587\n",
      "Iteration 1054 => Loss: 29.359820\n",
      "Iteration 1055 => Loss: 29.334253\n",
      "Iteration 1056 => Loss: 29.308887\n",
      "Iteration 1057 => Loss: 29.283720\n",
      "Iteration 1058 => Loss: 29.258753\n",
      "Iteration 1059 => Loss: 29.258447\n",
      "Iteration 1060 => Loss: 29.231147\n",
      "Iteration 1061 => Loss: 29.204047\n",
      "Iteration 1062 => Loss: 29.177147\n",
      "Iteration 1063 => Loss: 29.150447\n",
      "Iteration 1064 => Loss: 29.123947\n",
      "Iteration 1065 => Loss: 29.097647\n",
      "Iteration 1066 => Loss: 29.071547\n",
      "Iteration 1067 => Loss: 29.045647\n",
      "Iteration 1068 => Loss: 29.019947\n",
      "Iteration 1069 => Loss: 28.994447\n",
      "Iteration 1070 => Loss: 28.969147\n",
      "Iteration 1071 => Loss: 28.944047\n",
      "Iteration 1072 => Loss: 28.919147\n",
      "Iteration 1073 => Loss: 28.894447\n",
      "Iteration 1074 => Loss: 28.869947\n",
      "Iteration 1075 => Loss: 28.845647\n",
      "Iteration 1076 => Loss: 28.821547\n",
      "Iteration 1077 => Loss: 28.797647\n",
      "Iteration 1078 => Loss: 28.795500\n",
      "Iteration 1079 => Loss: 28.769267\n",
      "Iteration 1080 => Loss: 28.743233\n",
      "Iteration 1081 => Loss: 28.717400\n",
      "Iteration 1082 => Loss: 28.691767\n",
      "Iteration 1083 => Loss: 28.666333\n",
      "Iteration 1084 => Loss: 28.641100\n",
      "Iteration 1085 => Loss: 28.616067\n",
      "Iteration 1086 => Loss: 28.591233\n",
      "Iteration 1087 => Loss: 28.566600\n",
      "Iteration 1088 => Loss: 28.542167\n",
      "Iteration 1089 => Loss: 28.517933\n",
      "Iteration 1090 => Loss: 28.493900\n",
      "Iteration 1091 => Loss: 28.470067\n",
      "Iteration 1092 => Loss: 28.446433\n",
      "Iteration 1093 => Loss: 28.423000\n",
      "Iteration 1094 => Loss: 28.399767\n",
      "Iteration 1095 => Loss: 28.376733\n",
      "Iteration 1096 => Loss: 28.375280\n",
      "Iteration 1097 => Loss: 28.349913\n",
      "Iteration 1098 => Loss: 28.324747\n",
      "Iteration 1099 => Loss: 28.299780\n",
      "Iteration 1100 => Loss: 28.275013\n",
      "Iteration 1101 => Loss: 28.250447\n",
      "Iteration 1102 => Loss: 28.226080\n",
      "Iteration 1103 => Loss: 28.201913\n",
      "Iteration 1104 => Loss: 28.177947\n",
      "Iteration 1105 => Loss: 28.154180\n",
      "Iteration 1106 => Loss: 28.130613\n",
      "Iteration 1107 => Loss: 28.107247\n",
      "Iteration 1108 => Loss: 28.084080\n",
      "Iteration 1109 => Loss: 28.061113\n",
      "Iteration 1110 => Loss: 28.038347\n",
      "Iteration 1111 => Loss: 28.015780\n",
      "Iteration 1112 => Loss: 27.993413\n",
      "Iteration 1113 => Loss: 27.971247\n",
      "Iteration 1114 => Loss: 27.970487\n",
      "Iteration 1115 => Loss: 27.945987\n",
      "Iteration 1116 => Loss: 27.921687\n",
      "Iteration 1117 => Loss: 27.897587\n",
      "Iteration 1118 => Loss: 27.873687\n",
      "Iteration 1119 => Loss: 27.849987\n",
      "Iteration 1120 => Loss: 27.826487\n",
      "Iteration 1121 => Loss: 27.803187\n",
      "Iteration 1122 => Loss: 27.780087\n",
      "Iteration 1123 => Loss: 27.757187\n",
      "Iteration 1124 => Loss: 27.734487\n",
      "Iteration 1125 => Loss: 27.711987\n",
      "Iteration 1126 => Loss: 27.689687\n",
      "Iteration 1127 => Loss: 27.667587\n",
      "Iteration 1128 => Loss: 27.645687\n",
      "Iteration 1129 => Loss: 27.623987\n",
      "Iteration 1130 => Loss: 27.602487\n",
      "Iteration 1131 => Loss: 27.581187\n",
      "Iteration 1132 => Loss: 27.581120\n",
      "Iteration 1133 => Loss: 27.557487\n",
      "Iteration 1134 => Loss: 27.534053\n",
      "Iteration 1135 => Loss: 27.510820\n",
      "Iteration 1136 => Loss: 27.487787\n",
      "Iteration 1137 => Loss: 27.464953\n",
      "Iteration 1138 => Loss: 27.442320\n",
      "Iteration 1139 => Loss: 27.419887\n",
      "Iteration 1140 => Loss: 27.397653\n",
      "Iteration 1141 => Loss: 27.375620\n",
      "Iteration 1142 => Loss: 27.353787\n",
      "Iteration 1143 => Loss: 27.332153\n",
      "Iteration 1144 => Loss: 27.310720\n",
      "Iteration 1145 => Loss: 27.289487\n",
      "Iteration 1146 => Loss: 27.268453\n",
      "Iteration 1147 => Loss: 27.247620\n",
      "Iteration 1148 => Loss: 27.226987\n",
      "Iteration 1149 => Loss: 27.206553\n",
      "Iteration 1150 => Loss: 27.186320\n",
      "Iteration 1151 => Loss: 27.184413\n",
      "Iteration 1152 => Loss: 27.161847\n",
      "Iteration 1153 => Loss: 27.139480\n",
      "Iteration 1154 => Loss: 27.117313\n",
      "Iteration 1155 => Loss: 27.095347\n",
      "Iteration 1156 => Loss: 27.073580\n",
      "Iteration 1157 => Loss: 27.052013\n",
      "Iteration 1158 => Loss: 27.030647\n",
      "Iteration 1159 => Loss: 27.009480\n",
      "Iteration 1160 => Loss: 26.988513\n",
      "Iteration 1161 => Loss: 26.967747\n",
      "Iteration 1162 => Loss: 26.947180\n",
      "Iteration 1163 => Loss: 26.926813\n",
      "Iteration 1164 => Loss: 26.906647\n",
      "Iteration 1165 => Loss: 26.886680\n",
      "Iteration 1166 => Loss: 26.866913\n",
      "Iteration 1167 => Loss: 26.847347\n",
      "Iteration 1168 => Loss: 26.827980\n",
      "Iteration 1169 => Loss: 26.826767\n",
      "Iteration 1170 => Loss: 26.805067\n",
      "Iteration 1171 => Loss: 26.783567\n",
      "Iteration 1172 => Loss: 26.762267\n",
      "Iteration 1173 => Loss: 26.741167\n",
      "Iteration 1174 => Loss: 26.720267\n",
      "Iteration 1175 => Loss: 26.699567\n",
      "Iteration 1176 => Loss: 26.679067\n",
      "Iteration 1177 => Loss: 26.658767\n",
      "Iteration 1178 => Loss: 26.638667\n",
      "Iteration 1179 => Loss: 26.618767\n",
      "Iteration 1180 => Loss: 26.599067\n",
      "Iteration 1181 => Loss: 26.579567\n",
      "Iteration 1182 => Loss: 26.560267\n",
      "Iteration 1183 => Loss: 26.541167\n",
      "Iteration 1184 => Loss: 26.522267\n",
      "Iteration 1185 => Loss: 26.503567\n",
      "Iteration 1186 => Loss: 26.485067\n",
      "Iteration 1187 => Loss: 26.484547\n",
      "Iteration 1188 => Loss: 26.463713\n",
      "Iteration 1189 => Loss: 26.443080\n",
      "Iteration 1190 => Loss: 26.422647\n",
      "Iteration 1191 => Loss: 26.402413\n",
      "Iteration 1192 => Loss: 26.382380\n",
      "Iteration 1193 => Loss: 26.362547\n",
      "Iteration 1194 => Loss: 26.342913\n",
      "Iteration 1195 => Loss: 26.323480\n",
      "Iteration 1196 => Loss: 26.304247\n",
      "Iteration 1197 => Loss: 26.285213\n",
      "Iteration 1198 => Loss: 26.266380\n",
      "Iteration 1199 => Loss: 26.247747\n",
      "Iteration 1200 => Loss: 26.229313\n",
      "Iteration 1201 => Loss: 26.211080\n",
      "Iteration 1202 => Loss: 26.193047\n",
      "Iteration 1203 => Loss: 26.175213\n",
      "Iteration 1204 => Loss: 26.157580\n",
      "Iteration 1205 => Loss: 26.140147\n",
      "Iteration 1206 => Loss: 26.137787\n",
      "Iteration 1207 => Loss: 26.118020\n",
      "Iteration 1208 => Loss: 26.098453\n",
      "Iteration 1209 => Loss: 26.079087\n",
      "Iteration 1210 => Loss: 26.059920\n",
      "Iteration 1211 => Loss: 26.040953\n",
      "Iteration 1212 => Loss: 26.022187\n",
      "Iteration 1213 => Loss: 26.003620\n",
      "Iteration 1214 => Loss: 25.985253\n",
      "Iteration 1215 => Loss: 25.967087\n",
      "Iteration 1216 => Loss: 25.949120\n",
      "Iteration 1217 => Loss: 25.931353\n",
      "Iteration 1218 => Loss: 25.913787\n",
      "Iteration 1219 => Loss: 25.896420\n",
      "Iteration 1220 => Loss: 25.879253\n",
      "Iteration 1221 => Loss: 25.862287\n",
      "Iteration 1222 => Loss: 25.845520\n",
      "Iteration 1223 => Loss: 25.828953\n",
      "Iteration 1224 => Loss: 25.827287\n",
      "Iteration 1225 => Loss: 25.808387\n",
      "Iteration 1226 => Loss: 25.789687\n",
      "Iteration 1227 => Loss: 25.771187\n",
      "Iteration 1228 => Loss: 25.752887\n",
      "Iteration 1229 => Loss: 25.734787\n",
      "Iteration 1230 => Loss: 25.716887\n",
      "Iteration 1231 => Loss: 25.699187\n",
      "Iteration 1232 => Loss: 25.681687\n",
      "Iteration 1233 => Loss: 25.664387\n",
      "Iteration 1234 => Loss: 25.647287\n",
      "Iteration 1235 => Loss: 25.630387\n",
      "Iteration 1236 => Loss: 25.613687\n",
      "Iteration 1237 => Loss: 25.597187\n",
      "Iteration 1238 => Loss: 25.580887\n",
      "Iteration 1239 => Loss: 25.564787\n",
      "Iteration 1240 => Loss: 25.548887\n",
      "Iteration 1241 => Loss: 25.533187\n",
      "Iteration 1242 => Loss: 25.532213\n",
      "Iteration 1243 => Loss: 25.514180\n",
      "Iteration 1244 => Loss: 25.496347\n",
      "Iteration 1245 => Loss: 25.478713\n",
      "Iteration 1246 => Loss: 25.461280\n",
      "Iteration 1247 => Loss: 25.444047\n",
      "Iteration 1248 => Loss: 25.427013\n",
      "Iteration 1249 => Loss: 25.410180\n",
      "Iteration 1250 => Loss: 25.393547\n",
      "Iteration 1251 => Loss: 25.377113\n",
      "Iteration 1252 => Loss: 25.360880\n",
      "Iteration 1253 => Loss: 25.344847\n",
      "Iteration 1254 => Loss: 25.329013\n",
      "Iteration 1255 => Loss: 25.313380\n",
      "Iteration 1256 => Loss: 25.297947\n",
      "Iteration 1257 => Loss: 25.282713\n",
      "Iteration 1258 => Loss: 25.267680\n",
      "Iteration 1259 => Loss: 25.252847\n",
      "Iteration 1260 => Loss: 25.252567\n",
      "Iteration 1261 => Loss: 25.235400\n",
      "Iteration 1262 => Loss: 25.218433\n",
      "Iteration 1263 => Loss: 25.201667\n",
      "Iteration 1264 => Loss: 25.185100\n",
      "Iteration 1265 => Loss: 25.168733\n",
      "Iteration 1266 => Loss: 25.152567\n",
      "Iteration 1267 => Loss: 25.136600\n",
      "Iteration 1268 => Loss: 25.120833\n",
      "Iteration 1269 => Loss: 25.105267\n",
      "Iteration 1270 => Loss: 25.089900\n",
      "Iteration 1271 => Loss: 25.074733\n",
      "Iteration 1272 => Loss: 25.059767\n",
      "Iteration 1273 => Loss: 25.045000\n",
      "Iteration 1274 => Loss: 25.030433\n",
      "Iteration 1275 => Loss: 25.016067\n",
      "Iteration 1276 => Loss: 25.001900\n",
      "Iteration 1277 => Loss: 24.987933\n",
      "Iteration 1278 => Loss: 24.974167\n",
      "Iteration 1279 => Loss: 24.972047\n",
      "Iteration 1280 => Loss: 24.955947\n",
      "Iteration 1281 => Loss: 24.940047\n",
      "Iteration 1282 => Loss: 24.924347\n",
      "Iteration 1283 => Loss: 24.908847\n",
      "Iteration 1284 => Loss: 24.893547\n",
      "Iteration 1285 => Loss: 24.878447\n",
      "Iteration 1286 => Loss: 24.863547\n",
      "Iteration 1287 => Loss: 24.848847\n",
      "Iteration 1288 => Loss: 24.834347\n",
      "Iteration 1289 => Loss: 24.820047\n",
      "Iteration 1290 => Loss: 24.805947\n",
      "Iteration 1291 => Loss: 24.792047\n",
      "Iteration 1292 => Loss: 24.778347\n",
      "Iteration 1293 => Loss: 24.764847\n",
      "Iteration 1294 => Loss: 24.751547\n",
      "Iteration 1295 => Loss: 24.738447\n",
      "Iteration 1296 => Loss: 24.725547\n",
      "Iteration 1297 => Loss: 24.724120\n",
      "Iteration 1298 => Loss: 24.708887\n",
      "Iteration 1299 => Loss: 24.693853\n",
      "Iteration 1300 => Loss: 24.679020\n",
      "Iteration 1301 => Loss: 24.664387\n",
      "Iteration 1302 => Loss: 24.649953\n",
      "Iteration 1303 => Loss: 24.635720\n",
      "Iteration 1304 => Loss: 24.621687\n",
      "Iteration 1305 => Loss: 24.607853\n",
      "Iteration 1306 => Loss: 24.594220\n",
      "Iteration 1307 => Loss: 24.580787\n",
      "Iteration 1308 => Loss: 24.567553\n",
      "Iteration 1309 => Loss: 24.554520\n",
      "Iteration 1310 => Loss: 24.541687\n",
      "Iteration 1311 => Loss: 24.529053\n",
      "Iteration 1312 => Loss: 24.516620\n",
      "Iteration 1313 => Loss: 24.504387\n",
      "Iteration 1314 => Loss: 24.492353\n",
      "Iteration 1315 => Loss: 24.491620\n",
      "Iteration 1316 => Loss: 24.477253\n",
      "Iteration 1317 => Loss: 24.463087\n",
      "Iteration 1318 => Loss: 24.449120\n",
      "Iteration 1319 => Loss: 24.435353\n",
      "Iteration 1320 => Loss: 24.421787\n",
      "Iteration 1321 => Loss: 24.408420\n",
      "Iteration 1322 => Loss: 24.395253\n",
      "Iteration 1323 => Loss: 24.382287\n",
      "Iteration 1324 => Loss: 24.369520\n",
      "Iteration 1325 => Loss: 24.356953\n",
      "Iteration 1326 => Loss: 24.344587\n",
      "Iteration 1327 => Loss: 24.332420\n",
      "Iteration 1328 => Loss: 24.320453\n",
      "Iteration 1329 => Loss: 24.308687\n",
      "Iteration 1330 => Loss: 24.297120\n",
      "Iteration 1331 => Loss: 24.285753\n",
      "Iteration 1332 => Loss: 24.274587\n",
      "Iteration 1333 => Loss: 24.274547\n",
      "Iteration 1334 => Loss: 24.261047\n",
      "Iteration 1335 => Loss: 24.247747\n",
      "Iteration 1336 => Loss: 24.234647\n",
      "Iteration 1337 => Loss: 24.221747\n",
      "Iteration 1338 => Loss: 24.209047\n",
      "Iteration 1339 => Loss: 24.196547\n",
      "Iteration 1340 => Loss: 24.184247\n",
      "Iteration 1341 => Loss: 24.172147\n",
      "Iteration 1342 => Loss: 24.160247\n",
      "Iteration 1343 => Loss: 24.148547\n",
      "Iteration 1344 => Loss: 24.137047\n",
      "Iteration 1345 => Loss: 24.125747\n",
      "Iteration 1346 => Loss: 24.114647\n",
      "Iteration 1347 => Loss: 24.103747\n",
      "Iteration 1348 => Loss: 24.093047\n",
      "Iteration 1349 => Loss: 24.082547\n",
      "Iteration 1350 => Loss: 24.072247\n",
      "Iteration 1351 => Loss: 24.062147\n",
      "Iteration 1352 => Loss: 24.060267\n",
      "Iteration 1353 => Loss: 24.047833\n",
      "Iteration 1354 => Loss: 24.035600\n",
      "Iteration 1355 => Loss: 24.023567\n",
      "Iteration 1356 => Loss: 24.011733\n",
      "Iteration 1357 => Loss: 24.000100\n",
      "Iteration 1358 => Loss: 23.988667\n",
      "Iteration 1359 => Loss: 23.977433\n",
      "Iteration 1360 => Loss: 23.966400\n",
      "Iteration 1361 => Loss: 23.955567\n",
      "Iteration 1362 => Loss: 23.944933\n",
      "Iteration 1363 => Loss: 23.934500\n",
      "Iteration 1364 => Loss: 23.924267\n",
      "Iteration 1365 => Loss: 23.914233\n",
      "Iteration 1366 => Loss: 23.904400\n",
      "Iteration 1367 => Loss: 23.894767\n",
      "Iteration 1368 => Loss: 23.885333\n",
      "Iteration 1369 => Loss: 23.876100\n",
      "Iteration 1370 => Loss: 23.874913\n",
      "Iteration 1371 => Loss: 23.863347\n",
      "Iteration 1372 => Loss: 23.851980\n",
      "Iteration 1373 => Loss: 23.840813\n",
      "Iteration 1374 => Loss: 23.829847\n",
      "Iteration 1375 => Loss: 23.819080\n",
      "Iteration 1376 => Loss: 23.808513\n",
      "Iteration 1377 => Loss: 23.798147\n",
      "Iteration 1378 => Loss: 23.787980\n",
      "Iteration 1379 => Loss: 23.778013\n",
      "Iteration 1380 => Loss: 23.768247\n",
      "Iteration 1381 => Loss: 23.758680\n",
      "Iteration 1382 => Loss: 23.749313\n",
      "Iteration 1383 => Loss: 23.740147\n",
      "Iteration 1384 => Loss: 23.731180\n",
      "Iteration 1385 => Loss: 23.722413\n",
      "Iteration 1386 => Loss: 23.713847\n",
      "Iteration 1387 => Loss: 23.705480\n",
      "Iteration 1388 => Loss: 23.704987\n",
      "Iteration 1389 => Loss: 23.694287\n",
      "Iteration 1390 => Loss: 23.683787\n",
      "Iteration 1391 => Loss: 23.673487\n",
      "Iteration 1392 => Loss: 23.663387\n",
      "Iteration 1393 => Loss: 23.653487\n",
      "Iteration 1394 => Loss: 23.643787\n",
      "Iteration 1395 => Loss: 23.634287\n",
      "Iteration 1396 => Loss: 23.624987\n",
      "Iteration 1397 => Loss: 23.615887\n",
      "Iteration 1398 => Loss: 23.606987\n",
      "Iteration 1399 => Loss: 23.598287\n",
      "Iteration 1400 => Loss: 23.589787\n",
      "Iteration 1401 => Loss: 23.581487\n",
      "Iteration 1402 => Loss: 23.573387\n",
      "Iteration 1403 => Loss: 23.565487\n",
      "Iteration 1404 => Loss: 23.557787\n",
      "Iteration 1405 => Loss: 23.550287\n",
      "Iteration 1406 => Loss: 23.542987\n",
      "Iteration 1407 => Loss: 23.540653\n",
      "Iteration 1408 => Loss: 23.531020\n",
      "Iteration 1409 => Loss: 23.521587\n",
      "Iteration 1410 => Loss: 23.512353\n",
      "Iteration 1411 => Loss: 23.503320\n",
      "Iteration 1412 => Loss: 23.494487\n",
      "Iteration 1413 => Loss: 23.485853\n",
      "Iteration 1414 => Loss: 23.477420\n",
      "Iteration 1415 => Loss: 23.469187\n",
      "Iteration 1416 => Loss: 23.461153\n",
      "Iteration 1417 => Loss: 23.453320\n",
      "Iteration 1418 => Loss: 23.445687\n",
      "Iteration 1419 => Loss: 23.438253\n",
      "Iteration 1420 => Loss: 23.431020\n",
      "Iteration 1421 => Loss: 23.423987\n",
      "Iteration 1422 => Loss: 23.417153\n",
      "Iteration 1423 => Loss: 23.410520\n",
      "Iteration 1424 => Loss: 23.404087\n",
      "Iteration 1425 => Loss: 23.402447\n",
      "Iteration 1426 => Loss: 23.393680\n",
      "Iteration 1427 => Loss: 23.385113\n",
      "Iteration 1428 => Loss: 23.376747\n",
      "Iteration 1429 => Loss: 23.368580\n",
      "Iteration 1430 => Loss: 23.360613\n",
      "Iteration 1431 => Loss: 23.352847\n",
      "Iteration 1432 => Loss: 23.345280\n",
      "Iteration 1433 => Loss: 23.337913\n",
      "Iteration 1434 => Loss: 23.330747\n",
      "Iteration 1435 => Loss: 23.323780\n",
      "Iteration 1436 => Loss: 23.317013\n",
      "Iteration 1437 => Loss: 23.310447\n",
      "Iteration 1438 => Loss: 23.304080\n",
      "Iteration 1439 => Loss: 23.297913\n",
      "Iteration 1440 => Loss: 23.291947\n",
      "Iteration 1441 => Loss: 23.286180\n",
      "Iteration 1442 => Loss: 23.280613\n",
      "Iteration 1443 => Loss: 23.279667\n",
      "Iteration 1444 => Loss: 23.271767\n",
      "Iteration 1445 => Loss: 23.264067\n",
      "Iteration 1446 => Loss: 23.256567\n",
      "Iteration 1447 => Loss: 23.249267\n",
      "Iteration 1448 => Loss: 23.242167\n",
      "Iteration 1449 => Loss: 23.235267\n",
      "Iteration 1450 => Loss: 23.228567\n",
      "Iteration 1451 => Loss: 23.222067\n",
      "Iteration 1452 => Loss: 23.215767\n",
      "Iteration 1453 => Loss: 23.209667\n",
      "Iteration 1454 => Loss: 23.203767\n",
      "Iteration 1455 => Loss: 23.198067\n",
      "Iteration 1456 => Loss: 23.192567\n",
      "Iteration 1457 => Loss: 23.187267\n",
      "Iteration 1458 => Loss: 23.182167\n",
      "Iteration 1459 => Loss: 23.177267\n",
      "Iteration 1460 => Loss: 23.172567\n",
      "Iteration 1461 => Loss: 23.172313\n",
      "Iteration 1462 => Loss: 23.165280\n",
      "Iteration 1463 => Loss: 23.158447\n",
      "Iteration 1464 => Loss: 23.151813\n",
      "Iteration 1465 => Loss: 23.145380\n",
      "Iteration 1466 => Loss: 23.139147\n",
      "Iteration 1467 => Loss: 23.133113\n",
      "Iteration 1468 => Loss: 23.127280\n",
      "Iteration 1469 => Loss: 23.121647\n",
      "Iteration 1470 => Loss: 23.116213\n",
      "Iteration 1471 => Loss: 23.110980\n",
      "Iteration 1472 => Loss: 23.105947\n",
      "Iteration 1473 => Loss: 23.101113\n",
      "Iteration 1474 => Loss: 23.096480\n",
      "Iteration 1475 => Loss: 23.092047\n",
      "Iteration 1476 => Loss: 23.087813\n",
      "Iteration 1477 => Loss: 23.083780\n",
      "Iteration 1478 => Loss: 23.079947\n",
      "Iteration 1479 => Loss: 23.076313\n",
      "Iteration 1480 => Loss: 23.074220\n",
      "Iteration 1481 => Loss: 23.068253\n",
      "Iteration 1482 => Loss: 23.062487\n",
      "Iteration 1483 => Loss: 23.056920\n",
      "Iteration 1484 => Loss: 23.051553\n",
      "Iteration 1485 => Loss: 23.046387\n",
      "Iteration 1486 => Loss: 23.041420\n",
      "Iteration 1487 => Loss: 23.036653\n",
      "Iteration 1488 => Loss: 23.032087\n",
      "Iteration 1489 => Loss: 23.027720\n",
      "Iteration 1490 => Loss: 23.023553\n",
      "Iteration 1491 => Loss: 23.019587\n",
      "Iteration 1492 => Loss: 23.015820\n",
      "Iteration 1493 => Loss: 23.012253\n",
      "Iteration 1494 => Loss: 23.008887\n",
      "Iteration 1495 => Loss: 23.005720\n",
      "Iteration 1496 => Loss: 23.002753\n",
      "Iteration 1497 => Loss: 22.999987\n",
      "Iteration 1498 => Loss: 22.998587\n",
      "Iteration 1499 => Loss: 22.993487\n",
      "Iteration 1500 => Loss: 22.988587\n",
      "Iteration 1501 => Loss: 22.983887\n",
      "Iteration 1502 => Loss: 22.979387\n",
      "Iteration 1503 => Loss: 22.975087\n",
      "Iteration 1504 => Loss: 22.970987\n",
      "Iteration 1505 => Loss: 22.967087\n",
      "Iteration 1506 => Loss: 22.963387\n",
      "Iteration 1507 => Loss: 22.959887\n",
      "Iteration 1508 => Loss: 22.956587\n",
      "Iteration 1509 => Loss: 22.953487\n",
      "Iteration 1510 => Loss: 22.950587\n",
      "Iteration 1511 => Loss: 22.947887\n",
      "Iteration 1512 => Loss: 22.945387\n",
      "Iteration 1513 => Loss: 22.943087\n",
      "Iteration 1514 => Loss: 22.940987\n",
      "Iteration 1515 => Loss: 22.939087\n",
      "Iteration 1516 => Loss: 22.938380\n",
      "Iteration 1517 => Loss: 22.934147\n",
      "Iteration 1518 => Loss: 22.930113\n",
      "Iteration 1519 => Loss: 22.926280\n",
      "Iteration 1520 => Loss: 22.922647\n",
      "Iteration 1521 => Loss: 22.919213\n",
      "Iteration 1522 => Loss: 22.915980\n",
      "Iteration 1523 => Loss: 22.912947\n",
      "Iteration 1524 => Loss: 22.910113\n",
      "Iteration 1525 => Loss: 22.907480\n",
      "Iteration 1526 => Loss: 22.905047\n",
      "Iteration 1527 => Loss: 22.902813\n",
      "Iteration 1528 => Loss: 22.900780\n",
      "Iteration 1529 => Loss: 22.898947\n",
      "Iteration 1530 => Loss: 22.897313\n",
      "Iteration 1531 => Loss: 22.895880\n",
      "Iteration 1532 => Loss: 22.894647\n",
      "Iteration 1533 => Loss: 22.893613\n",
      "Iteration 1534 => Loss: 22.893600\n",
      "Iteration 1535 => Loss: 22.890233\n",
      "Iteration 1536 => Loss: 22.887067\n",
      "Iteration 1537 => Loss: 22.884100\n",
      "Iteration 1538 => Loss: 22.881333\n",
      "Iteration 1539 => Loss: 22.878767\n",
      "Iteration 1540 => Loss: 22.876400\n",
      "Iteration 1541 => Loss: 22.874233\n",
      "Iteration 1542 => Loss: 22.872267\n",
      "Iteration 1543 => Loss: 22.870500\n",
      "Iteration 1544 => Loss: 22.868933\n",
      "Iteration 1545 => Loss: 22.867567\n",
      "Iteration 1546 => Loss: 22.866400\n",
      "Iteration 1547 => Loss: 22.865433\n",
      "Iteration 1548 => Loss: 22.864667\n",
      "Iteration 1549 => Loss: 22.864100\n",
      "Iteration 1550 => Loss: 22.863733\n",
      "Iteration 1551 => Loss: 22.863567\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Here are the line's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1000000000000008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.929999999999769"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the line visualized over the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the examples like we did before:\n",
    "plt.xlabel(\"Reservations\", fontsize=20)\n",
    "plt.ylabel(\"Pizzas\", fontsize=20)\n",
    "plt.axis([0, 100, 0, 100])\n",
    "plt.plot(X, Y, \"bo\")\n",
    "\n",
    "# Plot the line:\n",
    "plt.plot([0, 50], [b, predict(50, w, b)], color=\"g\")\n",
    "\n",
    "# Visualize the diagram:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use this model to predict how many pizzas we're going to sell if we got 42 reservations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.1299999999998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 42\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should prepare enough dough for about 60 pizzas. We just built a system that learns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.12999999999981"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 52\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.92999999999982"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 80\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
